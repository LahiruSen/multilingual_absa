{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "from collections import namedtuple\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking data set from Seeval 2016 - task 5 subset 1 : http://alt.qcri.org/semeval2016/task5/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from collections import Counter, defaultdict\n",
    "import re\n",
    "import ast\n",
    "pd.set_option('display.max_colwidth' , -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### English - Restaurant domain training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aspects</th>\n",
       "      <th>polarities</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[RESTAURANT#GENERAL]</td>\n",
       "      <td>[negative]</td>\n",
       "      <td>Judging from previous posts this used to be a good place, but not any longer.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[SERVICE#GENERAL]</td>\n",
       "      <td>[negative]</td>\n",
       "      <td>We, there were four of us, arrived at noon - the place was empty - and the staff acted like we were imposing on them and they were very rude.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                aspects  polarities  \\\n",
       "0  [RESTAURANT#GENERAL]  [negative]   \n",
       "1  [SERVICE#GENERAL]     [negative]   \n",
       "\n",
       "                                                                                                                                            text  \n",
       "0  Judging from previous posts this used to be a good place, but not any longer.                                                                  \n",
       "1  We, there were four of us, arrived at noon - the place was empty - and the staff acted like we were imposing on them and they were very rude.  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_multi_aspects = pd.read_csv('../data/English_restaurants.csv')\n",
    "eng_multi_aspects['aspects'] = eng_multi_aspects['aspects'].apply(lambda x: ast.literal_eval(x))\n",
    "eng_multi_aspects['polarities'] = eng_multi_aspects['polarities'].apply(lambda x: ast.literal_eval(x))\n",
    "eng_multi_aspects.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FOOD#QUALITY                0.338652\n",
       "SERVICE#GENERAL             0.179099\n",
       "RESTAURANT#GENERAL          0.168329\n",
       "AMBIENCE#GENERAL            0.101715\n",
       "FOOD#STYLE_OPTIONS          0.054647\n",
       "RESTAURANT#MISCELLANEOUS    0.039091\n",
       "FOOD#PRICES                 0.035899\n",
       "RESTAURANT#PRICES           0.031911\n",
       "DRINKS#QUALITY              0.018748\n",
       "DRINKS#STYLE_OPTIONS        0.012764\n",
       "LOCATION#GENERAL            0.011169\n",
       "DRINKS#PRICES               0.007978\n",
       "Name: value, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_multi_aspects.aspects.apply(pd.Series).merge(eng_multi_aspects , right_index = True , left_index = True)\\\n",
    ".drop(['aspects' , 'polarities'] ,axis = 1).melt(id_vars = ['text']).drop(['variable'] , axis = 1).dropna().value.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dutch  - Restaurant domain training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aspects</th>\n",
       "      <th>polarities</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[SERVICE#GENERAL]</td>\n",
       "      <td>[negative]</td>\n",
       "      <td>Lange wachttijd.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[FOOD#STYLE_OPTIONS]</td>\n",
       "      <td>[negative]</td>\n",
       "      <td>Zelfde dessert, 2 dagen na mekaar.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                aspects  polarities                                text\n",
       "0  [SERVICE#GENERAL]     [negative]  Lange wachttijd.                  \n",
       "1  [FOOD#STYLE_OPTIONS]  [negative]  Zelfde dessert, 2 dagen na mekaar."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "du_multi_aspects = pd.read_csv('../data/Dutch_restaurants.csv')\n",
    "du_multi_aspects['aspects'] = du_multi_aspects['aspects'].apply(lambda x: ast.literal_eval(x))\n",
    "du_multi_aspects['polarities'] = du_multi_aspects['polarities'].apply(lambda x: ast.literal_eval(x))\n",
    "du_multi_aspects.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spanish Restaurant domain training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aspects</th>\n",
       "      <th>polarities</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[RESTAURANT#GENERAL]</td>\n",
       "      <td>[positive]</td>\n",
       "      <td>Nos sentimos muy a gusto.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[SERVICE#GENERAL, AMBIENCE#GENERAL, FOOD#QUALITY]</td>\n",
       "      <td>[positive, positive, positive]</td>\n",
       "      <td>Buen servicio, ambiente Acogedor  y tranquilo, comida bien.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             aspects  \\\n",
       "0  [RESTAURANT#GENERAL]                                \n",
       "1  [SERVICE#GENERAL, AMBIENCE#GENERAL, FOOD#QUALITY]   \n",
       "\n",
       "                       polarities  \\\n",
       "0  [positive]                       \n",
       "1  [positive, positive, positive]   \n",
       "\n",
       "                                                          text  \n",
       "0  Nos sentimos muy a gusto.                                    \n",
       "1  Buen servicio, ambiente Acogedor  y tranquilo, comida bien.  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spanish_multi_aspects = pd.read_csv('../data/Spanish_restaurants.csv')\n",
    "spanish_multi_aspects['aspects'] = spanish_multi_aspects['aspects'].apply(lambda x: ast.literal_eval(x))\n",
    "spanish_multi_aspects['polarities'] = spanish_multi_aspects['polarities'].apply(lambda x: ast.literal_eval(x))\n",
    "spanish_multi_aspects.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_aspects(lst_aspects):\n",
    "    res = []\n",
    "    for x in lst_aspects:\n",
    "        entity = x.split('#')[0] ; attribute = x.split('#')[1] ; res.append(entity)       \n",
    "    return res\n",
    "eng_multi_aspects['aspects2'] = eng_multi_aspects['aspects'].apply(lambda x: extract_aspects(x))\n",
    "eng_multi_aspects['aspects2'] = eng_multi_aspects['aspects2'].apply(lambda x : list(set(x)))\n",
    "\n",
    "du_multi_aspects['aspects2'] = du_multi_aspects['aspects'].apply(lambda x: extract_aspects(x))\n",
    "du_multi_aspects['aspects2'] = du_multi_aspects['aspects2'].apply(lambda x : list(set(x)))\n",
    "\n",
    "spanish_multi_aspects['aspects2'] = spanish_multi_aspects['aspects'].apply(lambda x: extract_aspects(x))\n",
    "spanish_multi_aspects['aspects2'] = spanish_multi_aspects['aspects2'].apply(lambda x : list(set(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_multi_aspects.aspects2.apply(pd.Series).merge(eng_multi_aspects , right_index = True , left_index = True)\\\n",
    ".drop(['aspects' , 'polarities', 'aspects2'] ,axis = 1).melt(id_vars = ['text']).drop(['variable'] , axis = 1).dropna().value.value_counts(normalize = True).reset_index()[['index']]\\\n",
    ".to_csv('../data/apsect_names.txt' , header = None , index = None , mode = 'w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract aspect embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "LASER_PATH = \"..\"\n",
    "sys.path.append(LASER_PATH + '/source')\n",
    "sys.path.append(LASER_PATH + '/source/lib')\n",
    "\n",
    "DATA_PATH = Path(\"../data/\")\n",
    "CACHE_PATH = Path(\"cache/\")\n",
    "CACHE_PATH.mkdir(exist_ok=True)\n",
    "MODEL_PATH = Path(\"../models\")\n",
    "\n",
    "os.environ[\"LASER\"] = LASER_PATH \n",
    "SPACE_NORMALIZER = re.compile(\"\\s+\")\n",
    "Batch = namedtuple('Batch', 'srcs tokens lengths')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from indexing import IndexLoad, IndexTextOpen, IndexTextQuery, IndexSearchKNN, IndexCreate, IndexSearchMultiple\n",
    "from embed import SentenceEncoder, EncodeLoad, EncodeFile\n",
    "from text_processing import Token, BPEfastApply"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Following steps from https://medium.com/the-artificial-impostor/multilingual-similarity-search-using-pretrained-bidirectional-lstm-encoder-e34fac5958b0 for tokenization , BPE Fast and Embedding extractions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Tokenizer: apsect_names.txt exists already\n",
      " - Tokenizer: apsect_names.txt exists already\n",
      " - Tokenizer: apsect_names.txt exists already\n"
     ]
    }
   ],
   "source": [
    "encoder = SentenceEncoder(\n",
    "    str(MODEL_PATH / \"bilstm.93langs.2018-12-26.pt\"),\n",
    "    max_sentences=None,\n",
    "    max_tokens=10000,\n",
    "    cpu=False)\n",
    "bpe_codes = str(MODEL_PATH / \"93langs.fcodes\")\n",
    "\n",
    "for lang in (\"en\" ,\"nl\", 'es'):  ##\"zh\" for chinese , nl  for dutch and es for spanish\n",
    "    Token(  #../data/apsect_names.txt\n",
    "        str(DATA_PATH / f\"apsect_names.txt\"), ##english_resturant.txt\n",
    "        str(CACHE_PATH / f\"apsect_names.txt\"),\n",
    "        lang=lang,\n",
    "        romanize=False,\n",
    "        lower_case=True, gzip=False,\n",
    "        verbose=True)\n",
    "    BPEfastApply(\n",
    "        str(CACHE_PATH / f\"apsect_names.txt\"),\n",
    "        str(CACHE_PATH / f\"apsect_names.bpe\"),\n",
    "        bpe_codes,\n",
    "        verbose=True, over_write=True)\n",
    "    EncodeFile(\n",
    "        encoder,\n",
    "        str(CACHE_PATH / f\"apsect_names.bpe\"),\n",
    "        str(CACHE_PATH / f\"apsect_names.enc\"),\n",
    "        verbose=True, over_write=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - embedding: cache/apsect_names.enc 6 examples of dim 1024\n",
      " - creating FAISS index\n"
     ]
    }
   ],
   "source": [
    "data_aspect, index_aspect = IndexCreate(\n",
    "     str(CACHE_PATH / f\"apsect_names.enc\"), 'FlatL2', verbose=True, save_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 1024)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_aspect.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Setence Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - embedding: cache/en_resturant.enc 1708 examples of dim 1024\n",
      " - creating FAISS index\n",
      " - embedding: cache/nl_resturant.enc 1317 examples of dim 1024\n",
      " - creating FAISS index\n",
      " - embedding: cache/es_resturant.enc 1626 examples of dim 1024\n",
      " - creating FAISS index\n"
     ]
    }
   ],
   "source": [
    "LASER_PATH = \"..\"\n",
    "sys.path.append(LASER_PATH + '/source')\n",
    "sys.path.append(LASER_PATH + '/source/lib')\n",
    "\n",
    "DATA_PATH = Path(\"../data/tatoeba/v1/\")\n",
    "CACHE_PATH = Path(\"cache/\")\n",
    "CACHE_PATH.mkdir(exist_ok=True)\n",
    "MODEL_PATH = Path(\"../models\")\n",
    "\n",
    "os.environ[\"LASER\"] = LASER_PATH \n",
    "SPACE_NORMALIZER = re.compile(\"\\s+\")\n",
    "Batch = namedtuple('Batch', 'srcs tokens lengths')\n",
    "\n",
    "data_en, index_en = IndexCreate(\n",
    "    str(CACHE_PATH / \"en_resturant.enc\"), 'FlatL2', verbose=True, save_index=False)\n",
    "data_du, index_du = IndexCreate(\n",
    "    str(CACHE_PATH / \"nl_resturant.enc\"), 'FlatL2', verbose=True, save_index=False)\n",
    "data_spanish, index_spanish = IndexCreate(\n",
    "    str(CACHE_PATH / \"es_resturant.enc\"), 'FlatL2', verbose=True, save_index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Because dataset of semeval is not exact translation of each other , some of the above results are not good. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating multi label classification task using LASER sentence embedding. \n",
    "We can have 6 aspect categories , present for each review. We will train a simple 1 layer Neural Network model using 1024 dimensional sentence embedding as input and 6 categories as output.  \n",
    "Train the model on 1700 English sentences and Validate on 1300 Dutch sentences . We are getting around 85 % accuracy and 57% f1 score(Macro) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarities</th>\n",
       "      <th>text</th>\n",
       "      <th>aspects2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[negative]</td>\n",
       "      <td>Judging from previous posts this used to be a good place, but not any longer.</td>\n",
       "      <td>[RESTAURANT]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[negative]</td>\n",
       "      <td>We, there were four of us, arrived at noon - the place was empty - and the staff acted like we were imposing on them and they were very rude.</td>\n",
       "      <td>[SERVICE]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   polarities  \\\n",
       "0  [negative]   \n",
       "1  [negative]   \n",
       "\n",
       "                                                                                                                                            text  \\\n",
       "0  Judging from previous posts this used to be a good place, but not any longer.                                                                   \n",
       "1  We, there were four of us, arrived at noon - the place was empty - and the staff acted like we were imposing on them and they were very rude.   \n",
       "\n",
       "       aspects2  \n",
       "0  [RESTAURANT]  \n",
       "1  [SERVICE]     "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_multi_aspects.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_multi_aspects.drop(columns=[ 'aspects'] , inplace=True)\n",
    "du_multi_aspects.drop(columns=[ 'aspects'] , inplace=True)\n",
    "spanish_multi_aspects.drop(columns=['aspects'] , inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarities</th>\n",
       "      <th>text</th>\n",
       "      <th>aspects2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[negative]</td>\n",
       "      <td>Judging from previous posts this used to be a good place, but not any longer.</td>\n",
       "      <td>[RESTAURANT]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[negative]</td>\n",
       "      <td>We, there were four of us, arrived at noon - the place was empty - and the staff acted like we were imposing on them and they were very rude.</td>\n",
       "      <td>[SERVICE]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[negative]</td>\n",
       "      <td>They never brought us complimentary noodles, ignored repeated requests for sugar, and threw our dishes on the table.</td>\n",
       "      <td>[SERVICE]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[negative, negative]</td>\n",
       "      <td>The food was lousy - too sweet or too salty and the portions tiny.</td>\n",
       "      <td>[FOOD]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[negative]</td>\n",
       "      <td>After all that, they complained to me about the small tip.</td>\n",
       "      <td>[SERVICE]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             polarities  \\\n",
       "0  [negative]             \n",
       "1  [negative]             \n",
       "2  [negative]             \n",
       "3  [negative, negative]   \n",
       "4  [negative]             \n",
       "\n",
       "                                                                                                                                            text  \\\n",
       "0  Judging from previous posts this used to be a good place, but not any longer.                                                                   \n",
       "1  We, there were four of us, arrived at noon - the place was empty - and the staff acted like we were imposing on them and they were very rude.   \n",
       "2  They never brought us complimentary noodles, ignored repeated requests for sugar, and threw our dishes on the table.                            \n",
       "3  The food was lousy - too sweet or too salty and the portions tiny.                                                                              \n",
       "4  After all that, they complained to me about the small tip.                                                                                      \n",
       "\n",
       "       aspects2  \n",
       "0  [RESTAURANT]  \n",
       "1  [SERVICE]     \n",
       "2  [SERVICE]     \n",
       "3  [FOOD]        \n",
       "4  [SERVICE]     "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_aspects , val_aspects, train_df , val_df = train_test_split(eng_multi_aspects, data_en , test_size = 0.2 , random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb  = MultiLabelBinarizer()\n",
    "tr_eng = mlb.fit_transform(train_aspects.aspects2)\n",
    "val_eng = mlb.transform(val_aspects.aspects2)\n",
    "y_du  = mlb.transform(du_multi_aspects.aspects2)\n",
    "y_spainish  = mlb.transform(spanish_multi_aspects.aspects2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_aspects.reset_index(inplace=True , drop= True)\n",
    "train_fn = pd.merge(train_aspects , pd.DataFrame(tr_eng , columns=mlb.classes_) , left_index=True , right_index=True)\n",
    "\n",
    "val_aspects.reset_index(inplace=True , drop= True)\n",
    "val_fn = pd.merge(val_aspects , pd.DataFrame(val_eng , columns=mlb.classes_) , left_index=True , right_index=True)\n",
    "\n",
    "train_fn.drop(columns=['aspects2' ] , inplace=True)\n",
    "\n",
    "# train_fn.to_csv('resturant_train_eng.csv' , index = False)\n",
    "# val_fn.to_csv('resturant_val_eng.csv' , index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "std_scale = StandardScaler().fit(train_df)\n",
    "train_std = std_scale.transform(train_df) \n",
    "val_std = std_scale.transform(val_df)\n",
    "dutch_std = std_scale.transform(data_du)\n",
    "spanish_std = std_scale.transform(data_spanish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1366, 1024), (342, 1024), (1317, 1024), (1626, 1024))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_std.shape , val_std.shape , dutch_std.shape , spanish_std.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1366, 6]) torch.Size([342, 6]) torch.Size([1317, 6])\n",
      "torch.Size([1366, 1024]) torch.Size([342, 1024]) torch.Size([1317, 6])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "x_train,y_train,x_valid,y_valid , x_test , y_test  , x_test_sp , y_test_sp = map(torch.FloatTensor, (train_std,tr_eng,  val_std ,\\\n",
    "                                                                            val_eng, dutch_std,y_du, \\\n",
    "                                                                           spanish_std ,y_spainish ))\n",
    "n,c = x_train.shape\n",
    "y_train = y_train.type(torch.FloatTensor)\n",
    "y_valid = y_valid.type(torch.FloatTensor)\n",
    "y_test = y_test.type(torch.FloatTensor)\n",
    "y_test_sp = y_test_sp.type(torch.FloatTensor)\n",
    "\n",
    "print(y_train.shape , y_valid.shape , y_test.shape)\n",
    "print(x_train.shape , x_valid.shape , y_test.shape)\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torch.nn.Sequential(\n",
    "#     torch.nn.Linear(1024, 512),\n",
    "#     torch.nn.Dropout(0.25),  # drop 10% of the neuron\n",
    "#     torch.nn.ReLU(),\n",
    "#     torch.nn.Linear(512, 384),\n",
    "#     torch.nn.Dropout(0.25),  # drop 10% of the neuron\n",
    "#     torch.nn.ReLU(),\n",
    "#     torch.nn.Linear(384, 6),\n",
    "# )\n",
    "\n",
    "# print(model)\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self , p):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(1024, 512)\n",
    "        self.hidden2 = nn.Linear(512 , 256)\n",
    "        self.hidden3 =  nn.Linear(256 , 128)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.fc = nn.Linear(128, 6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.dropout(self.hidden(x)))\n",
    "        x = self.activation(self.dropout(self.hidden2(x)))\n",
    "        x = self.activation(self.dropout(self.hidden3(x)))\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_ds = TensorDataset(x_train, y_train)\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size , shuffle=True)\n",
    "\n",
    "valid_ds = TensorDataset(x_valid, y_valid )\n",
    "valid_dl = DataLoader(valid_ds , batch_size= batch_size)\n",
    "\n",
    "test_ds = TensorDataset(x_test , y_test)\n",
    "test_dl = DataLoader(test_ds , batch_size=batch_size)\n",
    "\n",
    "test_ds2 = TensorDataset(x_test_sp , y_test_sp)\n",
    "test_dl2 = DataLoader(test_ds2 , batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrappedDataLoader():\n",
    "    def __init__(self, dl, func):\n",
    "        self.dl = dl\n",
    "        self.func = func\n",
    "        \n",
    "    def __len__(self): return len(self.dl)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        batches = iter(self.dl)\n",
    "        for b in batches: yield(self.func(*b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "def preprocess(x,y): return x.to(dev),y.to(dev)\n",
    "\n",
    "train_dl = WrappedDataLoader(train_dl, preprocess)\n",
    "valid_dl = WrappedDataLoader(valid_dl, preprocess)\n",
    "test_dl = WrappedDataLoader(test_dl , preprocess)\n",
    "test_dl2 = WrappedDataLoader(test_dl2 , preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_ratio = train_aspects.aspects2.apply(pd.Series).merge(train_aspects , right_index = True , left_index = True)\\\n",
    ".drop([ 'aspects2'] ,axis = 1).melt(id_vars = ['text']).drop(['variable'] , axis = 1).dropna().value.value_counts(normalize = True).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FOOD</td>\n",
       "      <td>0.361111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RESTAURANT</td>\n",
       "      <td>0.270531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SERVICE</td>\n",
       "      <td>0.198671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AMBIENCE</td>\n",
       "      <td>0.117754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DRINKS</td>\n",
       "      <td>0.038043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LOCATION</td>\n",
       "      <td>0.013889</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        index     value\n",
       "0  FOOD        0.361111\n",
       "1  RESTAURANT  0.270531\n",
       "2  SERVICE     0.198671\n",
       "3  AMBIENCE    0.117754\n",
       "4  DRINKS      0.038043\n",
       "5  LOCATION    0.013889"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def binary_accuracy(preds, y):\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.mean() #/ (len(correct))\n",
    "    return acc\n",
    "\n",
    "\n",
    "def fbeta_score(y_true, y_pred, beta, threshold, eps=1e-9):\n",
    "    beta2 = beta**2\n",
    "\n",
    "    y_pred = torch.ge(torch.sigmoid(y_pred).float(), threshold).float()\n",
    "    y_true = y_true.float()\n",
    "\n",
    "    true_positive = (y_pred * y_true).sum(dim=0)\n",
    "    precision = true_positive.div(y_pred.sum(dim=0).add(eps))\n",
    "    recall = true_positive.div(y_true.sum(dim=0).add(eps))\n",
    "    \n",
    "    return torch.mean(\n",
    "        (precision*recall).\n",
    "        div(precision.mul(beta2) + recall + eps).\n",
    "        mul(1 + beta2)) , torch.mean(precision) , torch.mean(recall)\n",
    "\n",
    "\n",
    "def f1_score(y_pred,y_true, threshold=0.5):\n",
    "    f1 , precision , recall = fbeta_score(y_true, y_pred, 1, threshold) #; print('f1 score' , f1)\n",
    "    return f1 , precision , recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0  \n",
    "    epoch_f1 = 0 ; epoch_precision = 0 ; epoch_recall = 0\n",
    "    model.train()\n",
    "    ct = 0\n",
    "    for x, y in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(x)\n",
    "        loss = criterion(predictions, y)\n",
    "        acc = binary_accuracy(predictions, y)\n",
    "        f1 , precision , recall = f1_score(predictions , y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        epoch_f1 += f1.item() \n",
    "        epoch_precision += precision.item()  \n",
    "        epoch_recall += recall.item()\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator) , epoch_f1/len(iterator), epoch_precision/len(iterator), epoch_recall/len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0 \n",
    "    epoch_f1 = 0; epoch_precision = 0 ; epoch_recall = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x ,y  in iterator:\n",
    "\n",
    "            predictions = model(x)#.squeeze(1)\n",
    "            loss = criterion(predictions,y)\n",
    "            acc = binary_accuracy(predictions, y) ; f1 , precision , recall = f1_score(predictions , y)\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "            epoch_f1 += f1.item()   ; epoch_precision += precision.item()  ; epoch_recall += recall.item()\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator) , epoch_f1/len(iterator), epoch_precision/len(iterator), epoch_recall/len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "        \n",
    "weight_list =  [1/df_data_ratio[df_data_ratio['index']==c]['value'].values[0]  for c in mlb.classes_]\n",
    "weights = torch.tensor( weight_list)\n",
    "weights =weights.to(dev)\n",
    "\n",
    "from torch import optim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply grid search on LR , Weight Decay , Dropout parameters , save the parameters with best f1-score on validatation data ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/FRACTAL/swati.tiwari/anaconda3/envs/fastai38/lib/python3.7/site-packages/ipykernel_launcher.py:3: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data 0.8562973656437614 0.3714965026487004 0.4546071710911664 0.3500112132592635\n",
      "valid data 0.8745659987131754 0.3745238035917282 0.4289509505033493 0.37096420923868817\n",
      "Parameters:  Dropout:  0.2 weight decay:  0.1  learning rate :  0.01\n",
      "chk\n",
      "train data 0.8555118116465482 0.38598748635162006 0.4850203611633994 0.3531473285772584\n",
      "valid data 0.8849431872367859 0.41120830178260803 0.5105532705783844 0.35407356917858124\n",
      "Parameters:  Dropout:  0.2 weight decay:  0.1  learning rate :  0.01\n",
      "chk\n",
      "train data 0.8760976439172571 0.478346202861179 0.6261945150115273 0.42765871503136377\n",
      "valid data 0.8880208631356558 0.47735116879145306 0.5990719298521677 0.4179966151714325\n",
      "Parameters:  Dropout:  0.2 weight decay:  0.1  learning rate :  0.005\n",
      "chk\n",
      "train data 0.882005436853929 0.48358452455563977 0.6040721914984963 0.4380798786878586\n",
      "valid data 0.8914930721124014 0.49457883834838867 0.5916983187198639 0.46525020400683087\n",
      "Parameters:  Dropout:  0.2 weight decay:  0.05  learning rate :  0.01\n",
      "chk\n",
      "train data 0.8767863484946165 0.5043848752975464 0.6458325331861322 0.4454274909062819\n",
      "valid data 0.9035274783770243 0.5143118798732758 0.6258809318145117 0.45876002311706543\n",
      "Parameters:  Dropout:  0.2 weight decay:  0.05  learning rate :  0.001\n",
      "chk\n",
      "train data 0.8876011723821814 0.5366301956501874 0.5992426723241806 0.509457756172527\n",
      "valid data 0.9109454154968262 0.5454176068305969 0.6239209075768789 0.5113721291224161\n",
      "Parameters:  Dropout:  0.3 weight decay:  0.01  learning rate :  0.005\n",
      "chk\n",
      "train data 0.8784865872426466 0.6044903085990385 0.6979317258704792 0.5604204074902968\n",
      "valid data 0.9144570827484131 0.5815974275271097 0.6596053044001261 0.5424553056557974\n",
      "Parameters:  Dropout:  0.5 weight decay:  0.01  learning rate :  0.001\n",
      "chk\n"
     ]
    }
   ],
   "source": [
    "best_valid_f1 = -float('inf') ; best_valid_loss = float('inf')\n",
    "loss_func = nn.BCEWithLogitsLoss(weight=weights) \n",
    "loss_func = loss_func.to(dev)\n",
    "for drp in [0.2, 0.3,0.4,0.5,0.6]:\n",
    "    for wd in [0.1 , 0.05 , 0.01 , 0.005 , 0.001]:\n",
    "        for learning_rate in [1e-2 , 5e-3 , 1e-3]:\n",
    "            model = Model(drp); model.apply(init_weights)\n",
    "            model = model.to(dev)\n",
    "            optimizer = optim.Adam(model.parameters() , lr = learning_rate, weight_decay=wd) #[a+'_pred' for a in aspects]\n",
    "            model = model.to(dev)\n",
    "            epochs = 10\n",
    "            for epoch in range(1, epochs + 1):\n",
    "                train_loss , train_acc , train_f1 , train_precision , train_recall = train_model(model, train_dl, optimizer, loss_func)\n",
    "                valid_loss , valid_acc , valid_f1 , valid_precision , valid_recall  = validate_model(model, valid_dl, loss_func)\n",
    "                if (valid_loss < best_valid_loss) & (valid_f1 > best_valid_f1)  & (abs(train_f1- valid_f1) <= 0.05):\n",
    "                    best_valid_f1 = valid_f1 ; best_valid_loss = valid_loss\n",
    "                    print('train data' , train_acc , train_f1 , train_precision , train_recall)\n",
    "                    print('valid data' , valid_acc ,  valid_f1 , valid_precision , valid_recall)\n",
    "\n",
    "\n",
    "                    print(\"Parameters: \" ,'Dropout: ' ,  drp , 'weight decay: ' ,wd ,' learning rate : ' ,learning_rate )\n",
    "                    if os.path.isfile('utils/multi_label_problem.pt'):\n",
    "                        os.remove('utils/multi_label_problem.pt') ; print('chk')\n",
    "                                           \n",
    "                    torch.save(model.state_dict(), 'utils/multi_label_problem.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.7336950500806172,\n",
       " 0.9144570827484131,\n",
       " 0.5815974275271097,\n",
       " 0.6596053044001261,\n",
       " 0.5424553056557974)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('utils/multi_label_problem.pt'))\n",
    "model = model.to(dev)\n",
    "validate_model(model, valid_dl, loss_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-67f96fad7f04>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score 0.6245610777652689\n",
      "Precision score 0.7023696749724148\n",
      "Recall score 0.5666044906639803\n",
      "Accuracy score 0.9132553606237817\n"
     ]
    }
   ],
   "source": [
    "val_preds = []\n",
    "val_label = []\n",
    "with torch.no_grad():\n",
    "    for x ,y  in valid_dl:\n",
    "        predictions = model(x)\n",
    "        rounded_preds = torch.round(torch.sigmoid(predictions))  #torch.round\n",
    "        preds = rounded_preds.data.cpu().numpy()\n",
    "        val_preds.append(preds)\n",
    "        val_label.append(y.data.cpu().numpy())\n",
    "\n",
    "from sklearn.metrics import f1_score , confusion_matrix , accuracy_score , precision_score , recall_score , roc_auc_score\n",
    "print(\"F1 score\",f1_score(np.vstack(val_label)  , np.vstack(val_preds) , average='macro' ))\n",
    "print(\"Precision score\",precision_score( np.vstack(val_label)  , np.vstack(val_preds) , average='macro' ))\n",
    "print(\"Recall score\",recall_score(np.vstack(val_label)  , np.vstack(val_preds) , average='macro' ))\n",
    "print(\"Accuracy score\" , np.mean( np.vstack(val_preds) == np.vstack(val_label)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = []\n",
    "true_label = []\n",
    "with torch.no_grad():\n",
    "    for x ,y  in test_dl:\n",
    "        predictions = model(x)#.squeeze(1)\n",
    "        rounded_preds = torch.round(torch.sigmoid(predictions))  #torch.round\n",
    "        preds = rounded_preds.data.cpu().numpy()\n",
    "        test_preds.append(preds)\n",
    "        true_label.append(y.data.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score 0.5464933540468624\n",
      "Accuracy score 0.8764869653252341\n",
      "Precision score 0.6988147708882922\n",
      "Recall score 0.48241459832236133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/FRACTAL/swati.tiwari/anaconda3/envs/fastai38/lib/python3.7/site-packages/ipykernel_launcher.py:10: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/home/FRACTAL/swati.tiwari/anaconda3/envs/fastai38/lib/python3.7/site-packages/ipykernel_launcher.py:11: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/home/FRACTAL/swati.tiwari/anaconda3/envs/fastai38/lib/python3.7/site-packages/ipykernel_launcher.py:12: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  if sys.path[0] == '':\n",
      "/home/FRACTAL/swati.tiwari/anaconda3/envs/fastai38/lib/python3.7/site-packages/ipykernel_launcher.py:13: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "aspects = mlb.classes_.tolist()\n",
    "\"\"\"\n",
    "Merging prediction value with original test data and observe the metrics on overall level\n",
    "\"\"\"\n",
    "dutch_pred = pd.DataFrame(np.vstack(test_preds) ,index=du_multi_aspects.index , columns= [a+'_pred' for a in aspects])\n",
    "dutch_pred2 = pd.merge(du_multi_aspects, dutch_pred , left_index=True ,right_index = True)\n",
    "\n",
    "from sklearn.metrics import f1_score , confusion_matrix , accuracy_score , precision_score , recall_score , roc_auc_score\n",
    "\n",
    "print(\"F1 score\",f1_score(y_du , dutch_pred2[[a+'_pred' for a in aspects]].as_matrix() , average='macro' ))\n",
    "print(\"Accuracy score\" , np.mean(y_du == dutch_pred2[[a+'_pred' for a in aspects]].as_matrix()))\n",
    "print(\"Precision score\",precision_score(y_du , dutch_pred2[[a+'_pred' for a in aspects]].as_matrix() , average='macro' ))\n",
    "print(\"Recall score\",recall_score(y_du , dutch_pred2[[a+'_pred' for a in aspects]].as_matrix() , average='macro' ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score 0.5536945586868443\n",
      "Accuracy score 0.8819188191881919\n",
      "Precision score 0.7265248425337454\n",
      "Recall score 0.47608903361016713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/FRACTAL/swati.tiwari/anaconda3/envs/fastai38/lib/python3.7/site-packages/ipykernel_launcher.py:21: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "/home/FRACTAL/swati.tiwari/anaconda3/envs/fastai38/lib/python3.7/site-packages/ipykernel_launcher.py:22: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "/home/FRACTAL/swati.tiwari/anaconda3/envs/fastai38/lib/python3.7/site-packages/ipykernel_launcher.py:23: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "/home/FRACTAL/swati.tiwari/anaconda3/envs/fastai38/lib/python3.7/site-packages/ipykernel_launcher.py:24: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    }
   ],
   "source": [
    "test_preds = []\n",
    "true_label = []\n",
    "with torch.no_grad():\n",
    "    for x ,y  in test_dl2:\n",
    "        predictions = model(x)#.squeeze(1)\n",
    "        rounded_preds = torch.round(torch.sigmoid(predictions))  #torch.round\n",
    "        preds = rounded_preds.data.cpu().numpy()\n",
    "        test_preds.append(preds)\n",
    "        true_label.append(y.data.cpu().numpy())\n",
    "        \n",
    "        \n",
    "aspects = mlb.classes_.tolist()\n",
    "\"\"\"\n",
    "Merging prediction value with original test data and observe the metrics on overall level\n",
    "\"\"\"\n",
    "spanish_pred = pd.DataFrame(np.vstack(test_preds) ,index=spanish_multi_aspects.index , columns= [a+'_pred' for a in aspects])\n",
    "spanish_pred2 = pd.merge(spanish_multi_aspects, spanish_pred , left_index=True ,right_index = True)\n",
    "\n",
    "from sklearn.metrics import f1_score , confusion_matrix , accuracy_score , precision_score , recall_score , roc_auc_score\n",
    "\n",
    "print(\"F1 score\",f1_score(y_spainish , spanish_pred2[[a+'_pred' for a in aspects]].as_matrix() , average='macro' ))\n",
    "print(\"Accuracy score\" , np.mean(y_spainish == spanish_pred2[[a+'_pred' for a in aspects]].as_matrix()))\n",
    "print(\"Precision score\",precision_score(y_spainish , spanish_pred2[[a+'_pred' for a in aspects]].as_matrix() , average='macro' ))\n",
    "print(\"Recall score\",recall_score(y_spainish , spanish_pred2[[a+'_pred' for a in aspects]].as_matrix() , average='macro' ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/FRACTAL/swati.tiwari/anaconda3/envs/fastai38/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "all_pred = dutch_pred2[dutch_pred2.columns[-6:]].as_matrix()\n",
    "all_true = y_du\n",
    "true_pos = (all_pred * all_true).sum(axis = 0)\n",
    "precision = true_pos/all_pred.sum(axis = 0)\n",
    "recall = true_pos/all_true.sum(axis = 0)\n",
    "accuracy = all_pred.sum(axis = 0)/ all_true.sum(axis =0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.68085106, 0.8490566 , 0.76570048, 0.20930233, 0.66349206,\n",
       "        0.86646884]),\n",
       " array([0.35955056, 0.48913043, 0.57952468, 0.33333333, 0.59206799,\n",
       "        0.6952381 ]))"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision , recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.47058824, 0.62068966, 0.65972945, 0.25714286, 0.6257485 ,\n",
       "       0.77146631])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(2*precision*recall / (precision + recall))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
