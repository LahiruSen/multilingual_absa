{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import collections\n",
    "import pickle\n",
    "import re\n",
    "import random\n",
    "import sys\n",
    "import os \n",
    "import time\n",
    "\n",
    "import gensim\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from gensim.models.fasttext import FastText\n",
    "from gensim.models import word2vec\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split,cross_val_score, cross_val_predict, KFold, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix,precision_recall_fscore_support\n",
    "from sklearn import svm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from numpy import cumsum\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras import initializers\n",
    "from keras.models import Sequential,Model,load_model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dropout, Activation, Flatten, \\\n",
    "    Embedding, Convolution1D, MaxPooling1D, AveragePooling1D, \\\n",
    "    Input, Dense, merge, Add,TimeDistributed, Bidirectional,SpatialDropout1D, \\\n",
    "    Concatenate, concatenate, multiply, Lambda, dot, Layer\n",
    "from keras.layers.recurrent import LSTM, GRU, SimpleRNN\n",
    "from keras.regularizers import l2, l1_l2\n",
    "from keras.constraints import maxnorm\n",
    "from keras import callbacks\n",
    "from keras.utils import generic_utils,plot_model\n",
    "from keras.optimizers import Adadelta,Adam\n",
    "from keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "# from keras.engine.topology import Layer\n",
    "\n",
    "\n",
    "import matplotlib.image  as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import gensim.downloader as gensimapi\n",
    "from collections import Counter, defaultdict\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Backup\r\n",
      "'Cross Lingual Aspect Based Sentiment Analysis- Part 1.ipynb'\r\n",
      "'Cross Lingual Aspect Based Sentiment Analysis- Part 2.ipynb'\r\n",
      " Keras_RNN.ipynb\r\n",
      " Keras_SVM.ipynb\r\n",
      "'Multilingual  semeval  dataset creation.ipynb'\r\n",
      " results\r\n",
      "'Tatoeba Similarity Search.ipynb'\r\n",
      " Twitter_Data.ipynb\r\n",
      " utils\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_folder = \"../../../corpus/new/preprocess_from_isuru/\"\n",
    "# lankadeepa_data_path = data_folder + 'lankadeepa_tagged_comments.csv'\n",
    "# gossip_lanka_data_path = data_folder + 'gossip_lanka_tagged_comments.csv'\n",
    "\n",
    "# lankadeepa_data = pd.read_csv(lankadeepa_data_path)[:9059]\n",
    "# gossipLanka_data = pd.read_csv(gossip_lanka_data_path)\n",
    "# gossipLanka_data = gossipLanka_data.drop(columns=['Unnamed: 3'])\n",
    "\n",
    "# sinhala_data = pd.concat([lankadeepa_data,gossipLanka_data], ignore_index=True)\n",
    "# sinhala_data.to_csv(\"../data/sinhala_comments.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_aspects_ungrp = pd.read_csv('../data/Train_english_restaurants_ungrp.csv')\n",
    "val_aspects_ungrp2 = pd.read_csv('../data/Valid_english_restaurants_ungrp.csv')\n",
    "du_aspects_ungrp2  = pd.read_csv('../data/Dutch_restaurants_ungrp.csv')\n",
    "sp_aspects_ungrp2  = pd.read_csv('../data/Spanish_english_restaurants_ungrp.csv')\n",
    "\n",
    "sinhala_comments  = pd.read_csv('../data/sinhala_comments.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sinhala_comments = sinhala_comments.loc[sinhala_comments['label'].isin([2, 3, 4])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2 7665]\n",
      " [   3 2403]\n",
      " [   4 3080]]\n"
     ]
    }
   ],
   "source": [
    "(unique, counts) = np.unique(sinhala_comments[\"label\"].values, return_counts = True)\n",
    "frequencies = np.asarray((unique, counts)).T\n",
    "print(frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_aspects_ungrp[['text']].to_csv('../data/processed/en_resturant.csv' , header = None , index = None , mode = 'w')\n",
    "val_aspects_ungrp2[['text']].to_csv('../data/processed/en_val.csv' , header = None , index = None , mode = 'w')\n",
    "du_aspects_ungrp2[['text']].to_csv('../data/processed/nl_resturant.csv' , header = None , index = None , mode = 'w')\n",
    "sp_aspects_ungrp2[['text']].to_csv('../data/processed/es_resturant.csv' , header = None , index = None , mode = 'w')\n",
    "\n",
    "sinhala_comments[['comment']].to_csv('../data/processed/sinhala_comments.csv' , header = None , index = None , mode = 'w')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "LASER_PATH = \"/home/lahiru/Projects/FYP/LASER/LASER\"\n",
    "sys.path.insert(0, LASER_PATH + '/source/lib')\n",
    "sys.path.insert(1, LASER_PATH + '/source')\n",
    "\n",
    "DATA_PATH = Path(\"../data/processed/\")\n",
    "CACHE_PATH = Path(\"../cache2/\")\n",
    "CACHE_PATH.mkdir(exist_ok=True)\n",
    "MODEL_PATH = Path(LASER_PATH + \"/models\")\n",
    "\n",
    "os.environ[\"LASER\"] = LASER_PATH \n",
    "SPACE_NORMALIZER = re.compile(\"\\s+\")\n",
    "Batch = namedtuple('Batch', 'srcs tokens lengths')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from indexing import IndexLoad, IndexTextOpen, IndexTextQuery, IndexSearchKNN, IndexCreate, IndexSearchMultiple\n",
    "from embed import SentenceEncoder, EncodeLoad, EncodeFile\n",
    "from text_processing import Token, BPEfastApply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Tokenizer: en_resturant.csv exists already\n",
      " - Tokenizer: nl_resturant.csv exists already\n",
      " - Tokenizer: es_resturant.csv exists already\n",
      " - Tokenizer: en_val.csv exists already\n"
     ]
    }
   ],
   "source": [
    "encoder = SentenceEncoder(\n",
    "    str(MODEL_PATH / \"bilstm.93langs.2018-12-26.pt\"),\n",
    "    max_sentences=None,\n",
    "    max_tokens=10000,\n",
    "    cpu=False)\n",
    "#original : cpu = False\n",
    "\n",
    "bpe_codes = str(MODEL_PATH / \"93langs.fcodes\")\n",
    "\n",
    "for lang in (\"en\" ,\"nl\", 'es'): \n",
    "    Token(\n",
    "        str(DATA_PATH / f\"{lang}_resturant.csv\"), ##english_resturant.txt\n",
    "        str(CACHE_PATH / f\"{lang}_resturant.csv\"),\n",
    "        lang=lang,\n",
    "        romanize=False,\n",
    "        lower_case=True, gzip=False,\n",
    "        verbose=True)\n",
    "    BPEfastApply(\n",
    "        str(CACHE_PATH / f\"{lang}_resturant.csv\"),\n",
    "        str(CACHE_PATH / f\"{lang}_resturant.bpe\"),\n",
    "        bpe_codes,\n",
    "        verbose=True, over_write=True)\n",
    "    EncodeFile(\n",
    "        encoder,\n",
    "        str(CACHE_PATH / f\"{lang}_resturant.bpe\"),\n",
    "        str(CACHE_PATH / f\"{lang}_resturant.enc\"),\n",
    "        verbose=True, over_write=True)    \n",
    "    \n",
    "    \n",
    "Token(\n",
    "    str(DATA_PATH / f\"en_val.csv\"), ##english_resturant.txt\n",
    "    str(CACHE_PATH / f\"en_val.csv\"),\n",
    "    lang=lang,\n",
    "    romanize=False,\n",
    "    lower_case=True, gzip=False,\n",
    "    verbose=True)\n",
    "BPEfastApply(\n",
    "    str(CACHE_PATH / f\"en_val.csv\"),\n",
    "    str(CACHE_PATH / f\"en_val.bpe\"),\n",
    "    bpe_codes,\n",
    "    verbose=True, over_write=True)\n",
    "EncodeFile(\n",
    "    encoder,\n",
    "    str(CACHE_PATH / f\"en_val.bpe\"),\n",
    "    str(CACHE_PATH / f\"en_val.enc\"),\n",
    "    verbose=True, over_write=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Tokenizer: sinhala_comments.csv exists already\n"
     ]
    }
   ],
   "source": [
    "Token(\n",
    "    str(DATA_PATH / f\"sinhala_comments.csv\"), ##english_resturant.txt\n",
    "    str(CACHE_PATH / f\"sinhala_comments.csv\"),\n",
    "    lang=\"sin\",\n",
    "    romanize=False,\n",
    "    lower_case=True, gzip=False,\n",
    "    verbose=True)\n",
    "BPEfastApply(\n",
    "    str(CACHE_PATH / f\"sinhala_comments.csv\"),\n",
    "    str(CACHE_PATH / f\"sinhala_comments.bpe\"),\n",
    "    bpe_codes,\n",
    "    verbose=True, over_write=True)\n",
    "EncodeFile(\n",
    "    encoder,\n",
    "    str(CACHE_PATH / f\"sinhala_comments.bpe\"),\n",
    "    str(CACHE_PATH / f\"sinhala_comments.enc\"),\n",
    "    verbose=True, over_write=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "LASER_PATH = \"/home/lahiru/Projects/FYP/LASER/LASER\"\n",
    "sys.path.insert(0, LASER_PATH + '/source/lib')\n",
    "sys.path.insert(1, LASER_PATH + '/source')\n",
    "\n",
    "DATA_PATH = Path(\"../data/processed/\")\n",
    "CACHE_PATH = Path(\"../cache2/\")\n",
    "CACHE_PATH.mkdir(exist_ok=True)\n",
    "MODEL_PATH = Path(LASER_PATH + \"/models\")\n",
    "\n",
    "os.environ[\"LASER\"] = LASER_PATH \n",
    "SPACE_NORMALIZER = re.compile(\"\\s+\")\n",
    "Batch = namedtuple('Batch', 'srcs tokens lengths')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - embedding: ../cache2/en_resturant.enc 1656 examples of dim 1024\n",
      " - creating FAISS index\n",
      " - embedding: ../cache2/en_val.enc 283 examples of dim 1024\n",
      " - creating FAISS index\n",
      " - embedding: ../cache2/nl_resturant.enc 960 examples of dim 1024\n",
      " - creating FAISS index\n",
      " - embedding: ../cache2/es_resturant.enc 1416 examples of dim 1024\n",
      " - creating FAISS index\n",
      " - embedding: ../cache2/sinhala_comments.enc 13148 examples of dim 1024\n",
      " - creating FAISS index\n"
     ]
    }
   ],
   "source": [
    "train_en, index_tr_en = IndexCreate(\n",
    "    str(CACHE_PATH / \"en_resturant.enc\"), 'FlatL2', verbose=True, save_index=False)\n",
    "val_en, index_val_en = IndexCreate(\n",
    "    str(CACHE_PATH / \"en_val.enc\"), 'FlatL2', verbose=True, save_index=False)\n",
    "\n",
    "data_du, index_du = IndexCreate(\n",
    "    str(CACHE_PATH / \"nl_resturant.enc\"), 'FlatL2', verbose=True, save_index=False)\n",
    "data_spanish, index_spanish = IndexCreate(\n",
    "    str(CACHE_PATH / \"es_resturant.enc\"), 'FlatL2', verbose=True, save_index=False)\n",
    "\n",
    "data_sinhala, index_sinhala = IndexCreate(\n",
    "    str(CACHE_PATH / \"sinhala_comments.enc\"), 'FlatL2', verbose=True, save_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_target(x):\n",
    "    if x=='positive':\n",
    "        return 2\n",
    "    elif x =='negative':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0 \n",
    "    \n",
    "def change_target_sinala(x):\n",
    "    if x== 4:\n",
    "        return 2\n",
    "    elif x == 2:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0 \n",
    "\n",
    "train_aspects_ungrp['polarities'] = train_aspects_ungrp['polarities'].apply(lambda x: change_target(x))\n",
    "val_aspects_ungrp2['polarities'] = val_aspects_ungrp2['polarities'].apply(lambda x: change_target(x))\n",
    "sp_aspects_ungrp2['polarities'] = sp_aspects_ungrp2['polarities'].apply(lambda x: change_target(x))\n",
    "du_aspects_ungrp2['polarities'] = du_aspects_ungrp2['polarities'].apply(lambda x: change_target(x))\n",
    "\n",
    "sinhala_comments['label'] = sinhala_comments['label'].apply(lambda x: change_target_sinala(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1656,), (283,), (960,), (1416,), (13148,))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "tr_eng =  train_aspects_ungrp['polarities'].values\n",
    "val_eng =val_aspects_ungrp2['polarities'].values  \n",
    "y_du  = du_aspects_ungrp2['polarities'].values\n",
    "y_spainish  = sp_aspects_ungrp2['polarities'].values\n",
    "\n",
    "y_sinhala  = sinhala_comments['label'].values\n",
    "tr_eng.shape , val_eng.shape , y_du.shape , y_spainish.shape, y_sinhala.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0 2403]\n",
      " [   1 7665]\n",
      " [   2 3080]]\n"
     ]
    }
   ],
   "source": [
    "(unique, counts) = np.unique(y_sinhala, return_counts = True)\n",
    "frequencies = np.asarray((unique, counts)).T\n",
    "print(frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "std_scale = StandardScaler().fit(train_en)\n",
    "train_std = std_scale.transform(train_en) \n",
    "val_std = std_scale.transform(val_en)\n",
    "dutch_std = std_scale.transform(data_du)\n",
    "spanish_std = std_scale.transform(data_spanish)\n",
    "sinhala_std = std_scale.transform(data_sinhala)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1656, 1024)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_std.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(sinhala_std, y_sinhala, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC(decision_function_shape='ovo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(decision_function_shape='ovo')"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(train_std, tr_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(solver='saga',\n",
    "                        multi_class='multinomial',\n",
    "                        penalty='elasticnet',\n",
    "                        l1_ratio = 0.1,\n",
    "                        max_iter=1000,\n",
    "                        random_state=42,\n",
    "                        verbose = 1,\n",
    "                        n_jobs = -1\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 955 epochs took 51 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   50.7s finished\n"
     ]
    }
   ],
   "source": [
    "lr_trained = lr.fit(train_std, tr_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = lr_trained.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Claculate Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.2000    0.0309    0.0535       453\n",
      "           1     0.6850    0.8415    0.7552      1571\n",
      "           2     0.5190    0.5396    0.5291       606\n",
      "\n",
      "    accuracy                         0.6323      2630\n",
      "   macro avg     0.4680    0.4707    0.4460      2630\n",
      "weighted avg     0.5632    0.6323    0.5823      2630\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report_print = classification_report(y_test, predictions, digits=4)\n",
    "print(report_print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
