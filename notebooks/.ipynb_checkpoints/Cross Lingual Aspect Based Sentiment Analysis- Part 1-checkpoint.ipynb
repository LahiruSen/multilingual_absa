{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lahiru/anaconda3/envs/LASER/lib/python3.6/site-packages/ipykernel_launcher.py:11: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from collections import Counter, defaultdict\n",
    "import ast\n",
    "pd.set_option('display.max_colwidth' , -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### English - Restaurant domain training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>aspects</th>\n",
       "      <th>polarities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Judging from previous posts this used to be a good place, but not any longer.</td>\n",
       "      <td>[RESTAURANT]</td>\n",
       "      <td>[negative]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We, there were four of us, arrived at noon - the place was empty - and the staff acted like we were imposing on them and they were very rude.</td>\n",
       "      <td>[SERVICE]</td>\n",
       "      <td>[negative]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                            text  \\\n",
       "0  Judging from previous posts this used to be a good place, but not any longer.                                                                   \n",
       "1  We, there were four of us, arrived at noon - the place was empty - and the staff acted like we were imposing on them and they were very rude.   \n",
       "\n",
       "        aspects  polarities  \n",
       "0  [RESTAURANT]  [negative]  \n",
       "1  [SERVICE]     [negative]  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_multi_aspects = pd.read_csv('../data/English_restaurants.csv')\n",
    "eng_multi_aspects['aspects'] = eng_multi_aspects['aspects'].apply(lambda x: ast.literal_eval(x))\n",
    "eng_multi_aspects['polarities'] = eng_multi_aspects['polarities'].apply(lambda x: ast.literal_eval(x))\n",
    "eng_multi_aspects.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FOOD          0.365171\n",
       "RESTAURANT    0.272069\n",
       "SERVICE       0.202123\n",
       "AMBIENCE      0.109021\n",
       "DRINKS        0.038109\n",
       "LOCATION      0.013507\n",
       "Name: value, dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_multi_aspects.aspects.apply(pd.Series).merge(eng_multi_aspects , right_index = True , left_index = True)\\\n",
    ".drop(['aspects' , 'polarities'] ,axis = 1).melt(id_vars = ['text']).drop(['variable'] , axis = 1).dropna().value.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dutch  - Restaurant domain training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>aspects</th>\n",
       "      <th>polarities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lange wachttijd.</td>\n",
       "      <td>[SERVICE]</td>\n",
       "      <td>[negative]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Zelfde dessert, 2 dagen na mekaar.</td>\n",
       "      <td>[FOOD]</td>\n",
       "      <td>[negative]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 text    aspects  polarities\n",
       "0  Lange wachttijd.                    [SERVICE]  [negative]\n",
       "1  Zelfde dessert, 2 dagen na mekaar.  [FOOD]     [negative]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "du_multi_aspects = pd.read_csv('../data/Dutch_restaurants.csv')\n",
    "du_multi_aspects['aspects'] = du_multi_aspects['aspects'].apply(lambda x: ast.literal_eval(x))\n",
    "du_multi_aspects['polarities'] = du_multi_aspects['polarities'].apply(lambda x: ast.literal_eval(x))\n",
    "du_multi_aspects.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spanish Restaurant domain training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>aspects</th>\n",
       "      <th>polarities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nos sentimos muy a gusto.</td>\n",
       "      <td>[RESTAURANT]</td>\n",
       "      <td>[positive]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Buen servicio, ambiente Acogedor  y tranquilo, comida bien.</td>\n",
       "      <td>[FOOD, SERVICE, AMBIENCE]</td>\n",
       "      <td>[positive, positive, positive]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                          text  \\\n",
       "0  Nos sentimos muy a gusto.                                     \n",
       "1  Buen servicio, ambiente Acogedor  y tranquilo, comida bien.   \n",
       "\n",
       "                     aspects                      polarities  \n",
       "0  [RESTAURANT]               [positive]                      \n",
       "1  [FOOD, SERVICE, AMBIENCE]  [positive, positive, positive]  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spanish_multi_aspects = pd.read_csv('../data/Spanish_restaurants.csv')\n",
    "spanish_multi_aspects['aspects'] = spanish_multi_aspects['aspects'].apply(lambda x: ast.literal_eval(x))\n",
    "spanish_multi_aspects['polarities'] = spanish_multi_aspects['polarities'].apply(lambda x: ast.literal_eval(x))\n",
    "spanish_multi_aspects.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_multi_aspects[['text']].to_csv('../data/processed/en_resturant.txt' , header = None , index = None , mode = 'w')\n",
    "du_multi_aspects[['text']].to_csv('../data/processed/nl_resturant.txt' , header = None , index = None , mode = 'w')\n",
    "spanish_multi_aspects[['text']].to_csv('../data/processed/es_resturant.txt' , header = None , index = None , mode = 'w')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '/home/lahiru/anaconda3/envs/LASER/lib/python36.zip', '/home/lahiru/anaconda3/envs/LASER/lib/python3.6', '/home/lahiru/anaconda3/envs/LASER/lib/python3.6/lib-dynload', '/home/lahiru/anaconda3/envs/LASER/lib/python3.6/site-packages', '/home/lahiru/anaconda3/envs/LASER/lib/python3.6/site-packages/fastBPE-0.1.1-py3.6-linux-x86_64.egg', '/home/lahiru/anaconda3/envs/LASER/lib/python3.6/site-packages/IPython/extensions', '/home/lahiru/.ipython']\n"
     ]
    }
   ],
   "source": [
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract sentence embeddings from text column of restaurant reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "LASER_PATH = \"/home/lahiru/Projects/FYP/LASER/LASER\"\n",
    "sys.path.insert(0, LASER_PATH + '/source/lib')\n",
    "sys.path.insert(1, LASER_PATH + '/source')\n",
    "\n",
    "\n",
    "DATA_PATH = Path(\"data/processed/\")\n",
    "CACHE_PATH = Path(\"cache/\")\n",
    "CACHE_PATH.mkdir(exist_ok=True)\n",
    "MODEL_PATH = Path(LASER_PATH + \"/models\")\n",
    "\n",
    "os.environ[\"LASER\"] = LASER_PATH \n",
    "SPACE_NORMALIZER = re.compile(\"\\s+\")\n",
    "Batch = namedtuple('Batch', 'srcs tokens lengths')\n",
    "\n",
    "# from indexing import IndexLoad, IndexTextOpen, IndexTextQuery, IndexSearchKNN, IndexCreate, IndexSearchMultiple\n",
    "from indexing import IndexTextOpen, IndexTextQuery, IndexSearchKNN, IndexCreate, IndexSearchMultiple\n",
    "from embed import SentenceEncoder, EncodeLoad, EncodeFile\n",
    "from text_processing import Token, BPEfastApply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = SentenceEncoder(\n",
    "    str(MODEL_PATH/\"bilstm.93langs.2018-12-26.pt\"),\n",
    "    max_sentences=None,\n",
    "    max_tokens=10000,\n",
    "    cpu=False)\n",
    "bpe_codes = str(MODEL_PATH/\"93langs.fcodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Following steps from https://medium.com/the-artificial-impostor/multilingual-similarity-search-using-pretrained-bidirectional-lstm-encoder-e34fac5958b0 for tokenization , BPE Fast and Embedding extractions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Tokenizer: en_resturant.txt exists already\n",
      " - Tokenizer: nl_resturant.txt exists already\n",
      " - Tokenizer: es_resturant.txt exists already\n",
      " - embedding: cache/en_resturant.enc 1708 examples of dim 1024\n",
      " - creating FAISS index\n",
      " - embedding: cache/nl_resturant.enc 1317 examples of dim 1024\n",
      " - creating FAISS index\n",
      " - embedding: cache/es_resturant.enc 1626 examples of dim 1024\n",
      " - creating FAISS index\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for lang in ( \"en\",\"nl\", 'es'):  ##\"zh\" for chinese , nl  for dutch and es for spanish\n",
    "    Token(\n",
    "        str(DATA_PATH / f\"{lang}_resturant.txt\"), ##english_resturant.txt\n",
    "        str(CACHE_PATH / f\"{lang}_resturant.txt\"),\n",
    "        lang=lang,\n",
    "        romanize=False,\n",
    "        lower_case=True, gzip=False,\n",
    "        verbose=True)\n",
    "    BPEfastApply(\n",
    "        str(CACHE_PATH / f\"{lang}_resturant.txt\"),\n",
    "        str(CACHE_PATH / f\"{lang}_resturant.bpe\"),\n",
    "        bpe_codes,\n",
    "        verbose=True, over_write=True)\n",
    "    EncodeFile(\n",
    "        encoder,\n",
    "        str(CACHE_PATH / f\"{lang}_resturant.bpe\"),\n",
    "        str(CACHE_PATH / f\"{lang}_resturant.enc\"),\n",
    "        verbose=True, over_write=True)  \n",
    "\n",
    "data_en, index_en = IndexCreate(\n",
    "    str(CACHE_PATH / \"en_resturant.enc\"), 'FlatL2', verbose=True, save_index=False)\n",
    "data_du, index_du = IndexCreate(\n",
    "    str(CACHE_PATH / \"nl_resturant.enc\"), 'FlatL2', verbose=True, save_index=False)\n",
    "data_spanish, index_spanish = IndexCreate(\n",
    "    str(CACHE_PATH / \"es_resturant.enc\"), 'FlatL2', verbose=True, save_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lahiru/Projects/FYP/LASER/multilingual_absa/notebooks\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating multi label classification task using sentence embeddings-  zero shot cross lingual training \n",
    "Training and hyper parameter optimisation in english language. Prediction in Dutch and Spanish language.\n",
    "Creating train valdiation split on English reviews. Creating target using MultiLabelBinarizer on Training Validation , Dutch and Spanish reviews. Normalizing the embeddings using StandardScaler (resulted in improvement in results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_aspects , val_aspects, train_df , val_df = train_test_split(eng_multi_aspects, data_en , test_size = 0.2 , random_state = 42)\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb  = MultiLabelBinarizer()\n",
    "tr_eng = mlb.fit_transform(train_aspects.aspects)\n",
    "val_eng = mlb.transform(val_aspects.aspects)\n",
    "y_du  = mlb.transform(du_multi_aspects.aspects)\n",
    "y_spainish  = mlb.transform(spanish_multi_aspects.aspects)\n",
    "\n",
    "train_aspects.reset_index(inplace=True , drop= True)\n",
    "val_aspects.reset_index(inplace=True , drop= True)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "std_scale = StandardScaler().fit(train_df)\n",
    "train_std = std_scale.transform(train_df) \n",
    "val_std = std_scale.transform(val_df)\n",
    "dutch_std = std_scale.transform(data_du)\n",
    "spanish_std = std_scale.transform(data_spanish)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating datasets for pytorch based multi label classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1366, 6]) torch.Size([342, 6]) torch.Size([1317, 6]) torch.Size([1626, 6])\n",
      "torch.Size([1366, 1024]) torch.Size([342, 1024]) torch.Size([1317, 1024]) torch.Size([1626, 1024])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "x_train,y_train,x_valid,y_valid , x_test , y_test  , x_test_sp , y_test_sp = map(torch.FloatTensor, (train_std,tr_eng,  val_std ,\\\n",
    "                                                                            val_eng, dutch_std,y_du, \\\n",
    "                                                                           spanish_std ,y_spainish ))\n",
    "n,c = x_train.shape\n",
    "y_train = y_train.type(torch.FloatTensor)\n",
    "y_valid = y_valid.type(torch.FloatTensor)\n",
    "y_test = y_test.type(torch.FloatTensor)\n",
    "y_test_sp = y_test_sp.type(torch.FloatTensor)\n",
    "\n",
    "print(y_train.shape , y_valid.shape , y_test.shape , y_test_sp.shape)\n",
    "print(x_train.shape , x_valid.shape , x_test.shape , x_test_sp.shape)\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self , p):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(1024, 512)\n",
    "        self.hidden2 = nn.Linear(512 , 256)\n",
    "        self.hidden3 =  nn.Linear(256 , 128)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.fc = nn.Linear(128, 6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.dropout(self.hidden(x)))\n",
    "        x = self.activation(self.dropout(self.hidden2(x)))\n",
    "        x = self.activation(self.dropout(self.hidden3(x)))\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_ds = TensorDataset(x_train, y_train)\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size , shuffle=True)\n",
    "\n",
    "valid_ds = TensorDataset(x_valid, y_valid )\n",
    "valid_dl = DataLoader(valid_ds , batch_size= batch_size)\n",
    "\n",
    "test_ds = TensorDataset(x_test , y_test)\n",
    "test_dl = DataLoader(test_ds , batch_size=batch_size)\n",
    "\n",
    "test_ds2 = TensorDataset(x_test_sp , y_test_sp)\n",
    "test_dl2 = DataLoader(test_ds2 , batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrappedDataLoader():\n",
    "    def __init__(self, dl, func):\n",
    "        self.dl = dl\n",
    "        self.func = func\n",
    "        \n",
    "    def __len__(self): return len(self.dl)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        batches = iter(self.dl)\n",
    "        for b in batches: yield(self.func(*b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "def preprocess(x,y): return x.to(dev),y.to(dev)\n",
    "\n",
    "train_dl = WrappedDataLoader(train_dl, preprocess)\n",
    "valid_dl = WrappedDataLoader(valid_dl, preprocess)\n",
    "test_dl = WrappedDataLoader(test_dl , preprocess)\n",
    "test_dl2 = WrappedDataLoader(test_dl2 , preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def binary_accuracy(preds, y):\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.mean() #/ (len(correct))\n",
    "    return acc\n",
    "\n",
    "\n",
    "def fbeta_score(y_true, y_pred, beta, threshold, eps=1e-9):\n",
    "    beta2 = beta**2\n",
    "\n",
    "    y_pred = torch.ge(torch.sigmoid(y_pred).float(), threshold).float()\n",
    "    y_true = y_true.float()\n",
    "\n",
    "    true_positive = (y_pred * y_true).sum(dim=0)\n",
    "    precision = true_positive.div(y_pred.sum(dim=0).add(eps))\n",
    "    recall = true_positive.div(y_true.sum(dim=0).add(eps))\n",
    "    \n",
    "    return torch.mean(\n",
    "        (precision*recall).\n",
    "        div(precision.mul(beta2) + recall + eps).\n",
    "        mul(1 + beta2)) , torch.mean(precision) , torch.mean(recall)\n",
    "\n",
    "\n",
    "def f1_score(y_pred,y_true, threshold=0.5):\n",
    "    f1 , precision , recall = fbeta_score(y_true, y_pred, 1, threshold) \n",
    "    return f1 , precision , recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0  \n",
    "    epoch_f1 = 0 ; epoch_precision = 0 ; epoch_recall = 0\n",
    "    model.train()\n",
    "    ct = 0\n",
    "    for x, y in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(x)\n",
    "        loss = criterion(predictions, y)\n",
    "        acc = binary_accuracy(predictions, y)\n",
    "        f1 , precision , recall = f1_score(predictions , y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        epoch_f1 += f1.item() \n",
    "        epoch_precision += precision.item()  \n",
    "        epoch_recall += recall.item()\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator) , epoch_f1/len(iterator), epoch_precision/len(iterator), epoch_recall/len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0 \n",
    "    epoch_f1 = 0; epoch_precision = 0 ; epoch_recall = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x ,y  in iterator:\n",
    "\n",
    "            predictions = model(x)#.squeeze(1)\n",
    "            loss = criterion(predictions,y)\n",
    "            acc = binary_accuracy(predictions, y) ; f1 , precision , recall = f1_score(predictions , y)\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "            epoch_f1 += f1.item()   ; epoch_precision += precision.item()  ; epoch_recall += recall.item()\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator) , epoch_f1/len(iterator), epoch_precision/len(iterator), epoch_recall/len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will use weight to scale loss value . It will result in giving equal weight to each category irrespective of data imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_ratio = train_aspects.aspects.apply(pd.Series).merge(train_aspects , right_index = True , left_index = True)\\\n",
    ".drop([ 'aspects' , 'polarities'] ,axis = 1).melt(id_vars = ['text']).drop(['variable'] , axis = 1).dropna().value.value_counts(normalize = True).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FOOD</td>\n",
       "      <td>0.361111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RESTAURANT</td>\n",
       "      <td>0.270531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SERVICE</td>\n",
       "      <td>0.198671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AMBIENCE</td>\n",
       "      <td>0.117754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DRINKS</td>\n",
       "      <td>0.038043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LOCATION</td>\n",
       "      <td>0.013889</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        index     value\n",
       "0  FOOD        0.361111\n",
       "1  RESTAURANT  0.270531\n",
       "2  SERVICE     0.198671\n",
       "3  AMBIENCE    0.117754\n",
       "4  DRINKS      0.038043\n",
       "5  LOCATION    0.013889"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "        \n",
    "weight_list =  [1/df_data_ratio[df_data_ratio['index']==c]['value'].values[0]  for c in mlb.classes_]\n",
    "weights = torch.tensor( weight_list)\n",
    "weights =weights.to(dev)\n",
    "\n",
    "from torch import optim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply grid search on Learning Rate, Weight Decay , Dropout parameters save the model with best validation loss and validation f1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lahiru/anaconda3/envs/LASER/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data 0.7844675454226407 0.21031122586943887 0.26326391236348584 0.2105742974037474\n",
      "valid data 0.8341224789619446 0.1986876701315244 0.20363339533408484 0.21564863373835882\n",
      "Parameters:  Dropout:  0.2 weight decay:  0.1  learning rate :  0.01\n",
      "train data 0.8503249856558713 0.3024580092592673 0.3898288682103157 0.27693889899687335\n",
      "valid data 0.8673058847586314 0.28538456559181213 0.3688344210386276 0.25794945160547894\n",
      "Parameters:  Dropout:  0.2 weight decay:  0.1  learning rate :  0.01\n",
      "train data 0.8517454402013258 0.3453310491009192 0.4657312482595444 0.32035766541957855\n",
      "valid data 0.8702651659647623 0.36802372833093006 0.49773676693439484 0.29698752611875534\n",
      "Parameters:  Dropout:  0.2 weight decay:  0.1  learning rate :  0.01\n",
      "train data 0.8683819743719968 0.48304555226456036 0.5830189003185793 0.4424492594870654\n",
      "valid data 0.894846906264623 0.5233721186717352 0.6097634881734848 0.4819316516319911\n",
      "Parameters:  Dropout:  0.2 weight decay:  0.1  learning rate :  0.005\n",
      "train data 0.8840177275917747 0.5194009596651251 0.6624286540529944 0.46440404518084094\n",
      "valid data 0.9013573129971822 0.5264168977737427 0.6238973091046015 0.48967018723487854\n",
      "Parameters:  Dropout:  0.2 weight decay:  0.01  learning rate :  0.001\n",
      "train data 0.8866003778847781 0.5492662773890928 0.6319552483883771 0.5178486379710111\n",
      "valid data 0.9079072078069051 0.5309042731920878 0.6195725202560425 0.4859903206427892\n",
      "Parameters:  Dropout:  0.3 weight decay:  0.01  learning rate :  0.005\n"
     ]
    }
   ],
   "source": [
    "best_valid_f1 = -float('inf') ; best_valid_loss = float('inf')\n",
    "loss_func = nn.BCEWithLogitsLoss(weight=weights) \n",
    "loss_func = loss_func.to(dev)\n",
    "for drp in [0.2, 0.3,0.4,0.5,0.6]:\n",
    "    for wd in [0.1 , 0.05 , 0.01 , 0.005 , 0.001]:\n",
    "        for learning_rate in [1e-2 , 5e-3 , 1e-3]:\n",
    "            model = Model(drp); model.apply(init_weights)\n",
    "            model = model.to(dev)\n",
    "            optimizer = optim.Adam(model.parameters() , lr = learning_rate, weight_decay=wd) #[a+'_pred' for a in aspects]\n",
    "            model = model.to(dev)\n",
    "            epochs = 10\n",
    "            for epoch in range(1, epochs + 1):\n",
    "                train_loss , train_acc , train_f1 , train_precision , train_recall = train_model(model, train_dl, optimizer, loss_func)\n",
    "                valid_loss , valid_acc , valid_f1 , valid_precision , valid_recall  = validate_model(model, valid_dl, loss_func)\n",
    "                if (valid_loss < best_valid_loss) & (valid_f1 > best_valid_f1)  & (abs(train_f1- valid_f1) <= 0.05):\n",
    "                    best_valid_f1 = valid_f1 ; best_valid_loss = valid_loss\n",
    "                    print('train data' , train_acc , train_f1 , train_precision , train_recall)\n",
    "                    print('valid data' , valid_acc ,  valid_f1 , valid_precision , valid_recall)\n",
    "\n",
    "\n",
    "                    print(\"Parameters: \" ,'Dropout: ' ,  drp , 'weight decay: ' ,wd ,' learning rate : ' ,learning_rate )\n",
    "                    if os.path.isfile('results/multi_label_problem.pt'):\n",
    "                        os.remove('results/multi_label_problem.pt') \n",
    "                                           \n",
    "                    torch.save(model.state_dict(), 'results/multi_label_problem.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.7675710320472717,\n",
       " 0.9079072078069051,\n",
       " 0.5309042731920878,\n",
       " 0.6195725202560425,\n",
       " 0.4859903206427892)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_func = nn.BCEWithLogitsLoss(weight=weights) \n",
    "loss_func = loss_func.to(dev)\n",
    "model = Model(0.5)\n",
    "model.load_state_dict(torch.load('results/multi_label_problem.pt'))\n",
    "model = model.to(dev)\n",
    "validate_model(model, valid_dl, loss_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate aspect prediction for validation dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lahiru/anaconda3/envs/LASER/lib/python3.6/site-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "val_preds = []\n",
    "val_label = []\n",
    "with torch.no_grad():\n",
    "    for x ,y  in valid_dl:\n",
    "        predictions = model(x)\n",
    "        rounded_preds = torch.round(torch.sigmoid(predictions))  #torch.round\n",
    "        preds = rounded_preds.data.cpu().numpy()\n",
    "        val_preds.append(preds)\n",
    "        val_label.append(y.data.cpu().numpy())\n",
    "\n",
    "val_aspects['aspects_pred'] = pd.Series(mlb.inverse_transform(np.vstack(val_preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate aspect prediction for dutch test dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'as_matrix'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-b8d0e7a82dad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mf1_score\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mconfusion_matrix\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0maccuracy_score\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mprecision_score\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mrecall_score\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"F1 score\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_du\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mdutch_pred2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_pred'\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maspects\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'macro'\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy score\"\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_du\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdutch_pred2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_pred'\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maspects\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Precision score\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprecision_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_du\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mdutch_pred2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_pred'\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maspects\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'macro'\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lahiru/anaconda3/envs/LASER/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5137\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5138\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5139\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5141\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'as_matrix'"
     ]
    }
   ],
   "source": [
    "test_preds = []\n",
    "true_label = []\n",
    "with torch.no_grad():\n",
    "    for x ,y  in test_dl:\n",
    "        predictions = model(x)#.squeeze(1)\n",
    "        rounded_preds = torch.round(torch.sigmoid(predictions))  #torch.round\n",
    "        preds = rounded_preds.data.cpu().numpy()\n",
    "        test_preds.append(preds)\n",
    "        true_label.append(y.data.cpu().numpy())\n",
    "        \n",
    "du_multi_aspects['aspects_pred']  = pd.Series(mlb.inverse_transform(np.vstack(test_preds)))\n",
    "\n",
    "aspects = mlb.classes_.tolist()\n",
    "\"\"\"\n",
    "Merging prediction value with original test data and observe the metrics on overall level\n",
    "\"\"\"\n",
    "dutch_pred = pd.DataFrame(np.vstack(test_preds) ,index=du_multi_aspects.index , columns= [a+'_pred' for a in aspects])\n",
    "dutch_pred2 = pd.merge(du_multi_aspects, dutch_pred , left_index=True ,right_index = True)\n",
    "\n",
    "from sklearn.metrics import f1_score , confusion_matrix , accuracy_score , precision_score , recall_score , roc_auc_score\n",
    "\n",
    "print(\"F1 score\",f1_score(y_du , dutch_pred2[[a+'_pred' for a in aspects]].as_matrix() , average='macro' ))\n",
    "print(\"Accuracy score\" , np.mean(y_du == dutch_pred2[[a+'_pred' for a in aspects]].as_matrix()))\n",
    "print(\"Precision score\",precision_score(y_du , dutch_pred2[[a+'_pred' for a in aspects]].as_matrix() , average='macro' ))\n",
    "print(\"Recall score\",recall_score(y_du , dutch_pred2[[a+'_pred' for a in aspects]].as_matrix() , average='macro' ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate aspect prediction for spanish test dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'as_matrix'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-1c4164860462>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mf1_score\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mconfusion_matrix\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0maccuracy_score\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mprecision_score\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mrecall_score\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"F1 score\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_spainish\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mspanish_pred2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_pred'\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maspects\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'macro'\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy score\"\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_spainish\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mspanish_pred2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_pred'\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maspects\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Precision score\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprecision_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_spainish\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mspanish_pred2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_pred'\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maspects\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'macro'\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lahiru/anaconda3/envs/LASER/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5137\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5138\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5139\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5141\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'as_matrix'"
     ]
    }
   ],
   "source": [
    "test_preds = []\n",
    "true_label = []\n",
    "with torch.no_grad():\n",
    "    for x ,y  in test_dl2:\n",
    "        predictions = model(x)#.squeeze(1)\n",
    "        rounded_preds = torch.round(torch.sigmoid(predictions))  #torch.round\n",
    "        preds = rounded_preds.data.cpu().numpy()\n",
    "        test_preds.append(preds)\n",
    "        true_label.append(y.data.cpu().numpy())\n",
    "        \n",
    "spanish_multi_aspects['aspects_pred']  = pd.Series(mlb.inverse_transform(np.vstack(test_preds)))\n",
    "\n",
    "        \n",
    "aspects = mlb.classes_.tolist()\n",
    "\"\"\"\n",
    "Merging prediction value with original test data and observe the metrics on overall level\n",
    "\"\"\"\n",
    "spanish_pred = pd.DataFrame(np.vstack(test_preds) ,index=spanish_multi_aspects.index , columns= [a+'_pred' for a in aspects])\n",
    "spanish_pred2 = pd.merge(spanish_multi_aspects, spanish_pred , left_index=True ,right_index = True)\n",
    "\n",
    "from sklearn.metrics import f1_score , confusion_matrix , accuracy_score , precision_score , recall_score , roc_auc_score\n",
    "\n",
    "print(\"F1 score\",f1_score(y_spainish , spanish_pred2[[a+'_pred' for a in aspects]].as_matrix() , average='macro' ))\n",
    "print(\"Accuracy score\" , np.mean(y_spainish == spanish_pred2[[a+'_pred' for a in aspects]].as_matrix()))\n",
    "print(\"Precision score\",precision_score(y_spainish , spanish_pred2[[a+'_pred' for a in aspects]].as_matrix() , average='macro' ))\n",
    "print(\"Recall score\",recall_score(y_spainish , spanish_pred2[[a+'_pred' for a in aspects]].as_matrix() , average='macro' ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next step is sentiment classification. We will work with only correct Text X Aspect results to predict sentiment.  Hence will will create an extra column aspects_pred for validation and test datasets.  Then we will filter out the correct  Text X Aspect combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lahiru/anaconda3/envs/LASER/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "val_aspects['aspects_pred'] = val_aspects['aspects_pred'].apply(lambda x: list(x))\n",
    "du_multi_aspects['aspects_pred'] = du_multi_aspects['aspects_pred'].apply(lambda x: list(x))\n",
    "spanish_multi_aspects['aspects_pred'] = spanish_multi_aspects['aspects_pred'].apply(lambda x: list(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ungrp(train_df):\n",
    "    asp_df = train_df.aspects.apply(pd.Series).merge(train_df , right_index = True , left_index = True)\\\n",
    "    .drop(['aspects' , 'polarities' , 'aspects_pred'] ,axis = 1)\\\n",
    "    .melt(id_vars = ['text']).drop(['variable'] , axis = 1).dropna()\n",
    "\n",
    "    polarity_df = train_df.polarities.apply(pd.Series).merge(train_df , right_index = True , left_index = True)\\\n",
    "    .drop(['aspects' , 'polarities' , 'aspects_pred'] ,axis = 1)\\\n",
    "    .melt(id_vars = ['text']).drop(['variable'] , axis = 1).dropna()\n",
    "\n",
    "    train_df_ungrp = pd.merge(asp_df , polarity_df['value'] , left_index = True , right_index = True ,suffixes=('_aspects' , '_polarities'))\n",
    "    train_df_ungrp.rename(columns={'value_aspects' : 'aspects' , 'value_polarities':'polarities'} , inplace=True)\n",
    "\n",
    "    train_df_ungrp2 = pd.merge(train_df_ungrp , train_df[[ 'text' ,'aspects_pred']] , on ='text')\n",
    "    return train_df_ungrp2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_aspects_ungrp  = ungrp(val_aspects)\n",
    "du_aspects_ungrp = ungrp(du_multi_aspects)\n",
    "sp_aspects_ungrp= ungrp(spanish_multi_aspects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1629, 4), (2321, 4), (417, 4))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "du_aspects_ungrp.shape , sp_aspects_ungrp.shape , val_aspects_ungrp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_ind(x):\n",
    "    asp = x['aspects_pred']\n",
    "    if x['aspects'] in asp:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_aspects_ungrp['ind'] = val_aspects_ungrp.apply(lambda x: check_ind(x) , axis = 1)\n",
    "du_aspects_ungrp['ind'] = du_aspects_ungrp.apply(lambda x: check_ind(x) , axis = 1)\n",
    "sp_aspects_ungrp['ind'] = sp_aspects_ungrp.apply(lambda x: check_ind(x) , axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6786570743405276, 0.5893186003683242, 0.6100818612666954)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_aspects_ungrp.ind.mean() , du_aspects_ungrp.ind.mean() , sp_aspects_ungrp.ind.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ungrp2(train_df):\n",
    "    asp_df = train_df.aspects.apply(pd.Series).merge(train_df , right_index = True , left_index = True)\\\n",
    "    .drop(['aspects' , 'polarities'] ,axis = 1)\\\n",
    "    .melt(id_vars = ['text']).drop(['variable'] , axis = 1).dropna()\n",
    "\n",
    "    polarity_df = train_df.polarities.apply(pd.Series).merge(train_df , right_index = True , left_index = True)\\\n",
    "    .drop(['aspects' , 'polarities'] ,axis = 1)\\\n",
    "    .melt(id_vars = ['text']).drop(['variable'] , axis = 1).dropna()\n",
    "\n",
    "    train_df_ungrp = pd.merge(asp_df , polarity_df['value'] , left_index = True , right_index = True ,suffixes=('_aspects' , '_polarities'))\n",
    "    train_df_ungrp.rename(columns={'value_aspects' : 'aspects' , 'value_polarities':'polarities'} , inplace=True)\n",
    "\n",
    "    return train_df_ungrp\n",
    "train_aspects_ungrp = ungrp2(train_aspects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>aspects</th>\n",
       "      <th>polarities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The food was very good, a great deal, and the place its self was great.</td>\n",
       "      <td>AMBIENCE</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Terrible would be a compliment!</td>\n",
       "      <td>RESTAURANT</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                      text  \\\n",
       "0  The food was very good, a great deal, and the place its self was great.   \n",
       "1  Terrible would be a compliment!                                           \n",
       "\n",
       "      aspects polarities  \n",
       "0  AMBIENCE    positive   \n",
       "1  RESTAURANT  negative   "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_aspects_ungrp.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see for the given text , we have been able to predict only 2 out 5 aspects correctly. We will filter out the text X aspects combination which our Multi Label Aspect is not able to predict correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>aspects</th>\n",
       "      <th>polarities</th>\n",
       "      <th>aspects_pred</th>\n",
       "      <th>ind</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Everything was wonderful; food, drinks, staff, mileau.</td>\n",
       "      <td>AMBIENCE</td>\n",
       "      <td>positive</td>\n",
       "      <td>[FOOD]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Everything was wonderful; food, drinks, staff, mileau.</td>\n",
       "      <td>SERVICE</td>\n",
       "      <td>positive</td>\n",
       "      <td>[FOOD]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Everything was wonderful; food, drinks, staff, mileau.</td>\n",
       "      <td>FOOD</td>\n",
       "      <td>positive</td>\n",
       "      <td>[FOOD]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Everything was wonderful; food, drinks, staff, mileau.</td>\n",
       "      <td>DRINKS</td>\n",
       "      <td>positive</td>\n",
       "      <td>[FOOD]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Everything was wonderful; food, drinks, staff, mileau.</td>\n",
       "      <td>RESTAURANT</td>\n",
       "      <td>positive</td>\n",
       "      <td>[FOOD]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text     aspects  \\\n",
       "20  Everything was wonderful; food, drinks, staff, mileau.  AMBIENCE     \n",
       "21  Everything was wonderful; food, drinks, staff, mileau.  SERVICE      \n",
       "22  Everything was wonderful; food, drinks, staff, mileau.  FOOD         \n",
       "23  Everything was wonderful; food, drinks, staff, mileau.  DRINKS       \n",
       "24  Everything was wonderful; food, drinks, staff, mileau.  RESTAURANT   \n",
       "\n",
       "   polarities aspects_pred  ind  \n",
       "20  positive   [FOOD]       0    \n",
       "21  positive   [FOOD]       0    \n",
       "22  positive   [FOOD]       1    \n",
       "23  positive   [FOOD]       0    \n",
       "24  positive   [FOOD]       0    "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_aspects_ungrp[val_aspects_ungrp['text']=='Everything was wonderful; food, drinks, staff, mileau.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_aspects_ungrp2 = val_aspects_ungrp[val_aspects_ungrp['ind']==1]\n",
    "du_aspects_ungrp2 = du_aspects_ungrp[du_aspects_ungrp['ind']==1]\n",
    "sp_aspects_ungrp2 = sp_aspects_ungrp[sp_aspects_ungrp['ind']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_aspects_ungrp.to_csv('../data/Train_english_restaurants_ungrp.csv' , index = False)\n",
    "val_aspects_ungrp2[['text', 'aspects', 'polarities']].to_csv('../data/Valid_english_restaurants_ungrp.csv' , index = False)\n",
    "du_aspects_ungrp2[['text', 'aspects', 'polarities']].to_csv('../data/Dutch_restaurants_ungrp.csv' , index = False)\n",
    "sp_aspects_ungrp2[['text', 'aspects', 'polarities']].to_csv('../data/Spanish_english_restaurants_ungrp.csv' , index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
