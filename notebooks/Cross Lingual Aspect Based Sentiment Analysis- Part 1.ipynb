{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from collections import Counter, defaultdict\n",
    "import ast\n",
    "pd.set_option('display.max_colwidth' , -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### English - Restaurant domain training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>aspects</th>\n",
       "      <th>polarities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Judging from previous posts this used to be a good place, but not any longer.</td>\n",
       "      <td>[RESTAURANT]</td>\n",
       "      <td>[negative]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We, there were four of us, arrived at noon - the place was empty - and the staff acted like we were imposing on them and they were very rude.</td>\n",
       "      <td>[SERVICE]</td>\n",
       "      <td>[negative]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                            text  \\\n",
       "0  Judging from previous posts this used to be a good place, but not any longer.                                                                   \n",
       "1  We, there were four of us, arrived at noon - the place was empty - and the staff acted like we were imposing on them and they were very rude.   \n",
       "\n",
       "        aspects  polarities  \n",
       "0  [RESTAURANT]  [negative]  \n",
       "1  [SERVICE]     [negative]  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_multi_aspects = pd.read_csv('../data/English_restaurants.csv')\n",
    "eng_multi_aspects['aspects'] = eng_multi_aspects['aspects'].apply(lambda x: ast.literal_eval(x))\n",
    "eng_multi_aspects['polarities'] = eng_multi_aspects['polarities'].apply(lambda x: ast.literal_eval(x))\n",
    "eng_multi_aspects.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FOOD          0.365171\n",
       "RESTAURANT    0.272069\n",
       "SERVICE       0.202123\n",
       "AMBIENCE      0.109021\n",
       "DRINKS        0.038109\n",
       "LOCATION      0.013507\n",
       "Name: value, dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_multi_aspects.aspects.apply(pd.Series).merge(eng_multi_aspects , right_index = True , left_index = True)\\\n",
    ".drop(['aspects' , 'polarities'] ,axis = 1).melt(id_vars = ['text']).drop(['variable'] , axis = 1).dropna().value.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dutch  - Restaurant domain training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>aspects</th>\n",
       "      <th>polarities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lange wachttijd.</td>\n",
       "      <td>[SERVICE]</td>\n",
       "      <td>[negative]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Zelfde dessert, 2 dagen na mekaar.</td>\n",
       "      <td>[FOOD]</td>\n",
       "      <td>[negative]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 text    aspects  polarities\n",
       "0  Lange wachttijd.                    [SERVICE]  [negative]\n",
       "1  Zelfde dessert, 2 dagen na mekaar.  [FOOD]     [negative]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "du_multi_aspects = pd.read_csv('../data/Dutch_restaurants.csv')\n",
    "du_multi_aspects['aspects'] = du_multi_aspects['aspects'].apply(lambda x: ast.literal_eval(x))\n",
    "du_multi_aspects['polarities'] = du_multi_aspects['polarities'].apply(lambda x: ast.literal_eval(x))\n",
    "du_multi_aspects.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spanish Restaurant domain training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>aspects</th>\n",
       "      <th>polarities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nos sentimos muy a gusto.</td>\n",
       "      <td>[RESTAURANT]</td>\n",
       "      <td>[positive]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Buen servicio, ambiente Acogedor  y tranquilo, comida bien.</td>\n",
       "      <td>[FOOD, SERVICE, AMBIENCE]</td>\n",
       "      <td>[positive, positive, positive]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                          text  \\\n",
       "0  Nos sentimos muy a gusto.                                     \n",
       "1  Buen servicio, ambiente Acogedor  y tranquilo, comida bien.   \n",
       "\n",
       "                     aspects                      polarities  \n",
       "0  [RESTAURANT]               [positive]                      \n",
       "1  [FOOD, SERVICE, AMBIENCE]  [positive, positive, positive]  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spanish_multi_aspects = pd.read_csv('../data/Spanish_restaurants.csv')\n",
    "spanish_multi_aspects['aspects'] = spanish_multi_aspects['aspects'].apply(lambda x: ast.literal_eval(x))\n",
    "spanish_multi_aspects['polarities'] = spanish_multi_aspects['polarities'].apply(lambda x: ast.literal_eval(x))\n",
    "spanish_multi_aspects.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_multi_aspects[['text']].to_csv('../data/processed/en_resturant.txt' , header = None , index = None , mode = 'w')\n",
    "du_multi_aspects[['text']].to_csv('../data/processed/nl_resturant.txt' , header = None , index = None , mode = 'w')\n",
    "spanish_multi_aspects[['text']].to_csv('../data/processed/es_resturant.txt' , header = None , index = None , mode = 'w')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract sentence embeddings from text column of restaurant reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "LASER_PATH = \"..\"\n",
    "sys.path.append(LASER_PATH + '/source')\n",
    "sys.path.append(LASER_PATH + '/source/lib')\n",
    "\n",
    "DATA_PATH = Path(\"../data/processed/\")\n",
    "CACHE_PATH = Path(\"cache/\")\n",
    "CACHE_PATH.mkdir(exist_ok=True)\n",
    "MODEL_PATH = Path(\"../models\")\n",
    "\n",
    "os.environ[\"LASER\"] = LASER_PATH \n",
    "SPACE_NORMALIZER = re.compile(\"\\s+\")\n",
    "Batch = namedtuple('Batch', 'srcs tokens lengths')\n",
    "\n",
    "from indexing import IndexLoad, IndexTextOpen, IndexTextQuery, IndexSearchKNN, IndexCreate, IndexSearchMultiple\n",
    "from embed import SentenceEncoder, EncodeLoad, EncodeFile\n",
    "from text_processing import Token, BPEfastApply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = SentenceEncoder(\n",
    "    str(MODEL_PATH/\"bilstm.93langs.2018-12-26.pt\"),\n",
    "    max_sentences=None,\n",
    "    max_tokens=10000,\n",
    "    cpu=False)\n",
    "bpe_codes = str(MODEL_PATH/\"93langs.fcodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Following steps from https://medium.com/the-artificial-impostor/multilingual-similarity-search-using-pretrained-bidirectional-lstm-encoder-e34fac5958b0 for tokenization , BPE Fast and Embedding extractions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Tokenizer: en_resturant.txt in language en  \n",
      " - fast BPE: processing en_resturant.txt\n",
      " - Encoder: en_resturant.bpe to en_resturant.enc\n",
      " - Encoder: 1708 sentences in 0s\n",
      " - Tokenizer: nl_resturant.txt in language nl  \n",
      " - fast BPE: processing nl_resturant.txt\n",
      " - Encoder: nl_resturant.bpe to nl_resturant.enc\n",
      " - Encoder: 1317 sentences in 0s\n",
      " - Tokenizer: es_resturant.txt in language es  \n",
      " - fast BPE: processing es_resturant.txt\n",
      " - Encoder: es_resturant.bpe to es_resturant.enc\n",
      " - Encoder: 1626 sentences in 0s\n",
      " - embedding: cache/en_resturant.enc 1708 examples of dim 1024\n",
      " - creating FAISS index\n",
      " - embedding: cache/nl_resturant.enc 1317 examples of dim 1024\n",
      " - creating FAISS index\n",
      " - embedding: cache/es_resturant.enc 1626 examples of dim 1024\n",
      " - creating FAISS index\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for lang in ( \"en\",\"nl\", 'es'):  ##\"zh\" for chinese , nl  for dutch and es for spanish\n",
    "    Token(\n",
    "        str(DATA_PATH / f\"{lang}_resturant.txt\"), ##english_resturant.txt\n",
    "        str(CACHE_PATH / f\"{lang}_resturant.txt\"),\n",
    "        lang=lang,\n",
    "        romanize=False,\n",
    "        lower_case=True, gzip=False,\n",
    "        verbose=True)\n",
    "    BPEfastApply(\n",
    "        str(CACHE_PATH / f\"{lang}_resturant.txt\"),\n",
    "        str(CACHE_PATH / f\"{lang}_resturant.bpe\"),\n",
    "        bpe_codes,\n",
    "        verbose=True, over_write=True)\n",
    "    EncodeFile(\n",
    "        encoder,\n",
    "        str(CACHE_PATH / f\"{lang}_resturant.bpe\"),\n",
    "        str(CACHE_PATH / f\"{lang}_resturant.enc\"),\n",
    "        verbose=True, over_write=True)  \n",
    "\n",
    "data_en, index_en = IndexCreate(\n",
    "    str(CACHE_PATH / \"en_resturant.enc\"), 'FlatL2', verbose=True, save_index=False)\n",
    "data_du, index_du = IndexCreate(\n",
    "    str(CACHE_PATH / \"nl_resturant.enc\"), 'FlatL2', verbose=True, save_index=False)\n",
    "data_spanish, index_spanish = IndexCreate(\n",
    "    str(CACHE_PATH / \"es_resturant.enc\"), 'FlatL2', verbose=True, save_index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating multi label classification task using sentence embeddings-  zero shot cross lingual training \n",
    "Training and hyper parameter optimisation in english language. Prediction in Dutch and Spanish language.\n",
    "Creating train valdiation split on English reviews. Creating target using MultiLabelBinarizer on Training Validation , Dutch and Spanish reviews. Normalizing the embeddings using StandardScaler (resulted in improvement in results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_aspects , val_aspects, train_df , val_df = train_test_split(eng_multi_aspects, data_en , test_size = 0.2 , random_state = 42)\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb  = MultiLabelBinarizer()\n",
    "tr_eng = mlb.fit_transform(train_aspects.aspects)\n",
    "val_eng = mlb.transform(val_aspects.aspects)\n",
    "y_du  = mlb.transform(du_multi_aspects.aspects)\n",
    "y_spainish  = mlb.transform(spanish_multi_aspects.aspects)\n",
    "\n",
    "train_aspects.reset_index(inplace=True , drop= True)\n",
    "val_aspects.reset_index(inplace=True , drop= True)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "std_scale = StandardScaler().fit(train_df)\n",
    "train_std = std_scale.transform(train_df) \n",
    "val_std = std_scale.transform(val_df)\n",
    "dutch_std = std_scale.transform(data_du)\n",
    "spanish_std = std_scale.transform(data_spanish)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating datasets for pytorch based multi label classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1366, 6]) torch.Size([342, 6]) torch.Size([1317, 6]) torch.Size([1626, 6])\n",
      "torch.Size([1366, 1024]) torch.Size([342, 1024]) torch.Size([1317, 1024]) torch.Size([1626, 1024])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "x_train,y_train,x_valid,y_valid , x_test , y_test  , x_test_sp , y_test_sp = map(torch.FloatTensor, (train_std,tr_eng,  val_std ,\\\n",
    "                                                                            val_eng, dutch_std,y_du, \\\n",
    "                                                                           spanish_std ,y_spainish ))\n",
    "n,c = x_train.shape\n",
    "y_train = y_train.type(torch.FloatTensor)\n",
    "y_valid = y_valid.type(torch.FloatTensor)\n",
    "y_test = y_test.type(torch.FloatTensor)\n",
    "y_test_sp = y_test_sp.type(torch.FloatTensor)\n",
    "\n",
    "print(y_train.shape , y_valid.shape , y_test.shape , y_test_sp.shape)\n",
    "print(x_train.shape , x_valid.shape , x_test.shape , x_test_sp.shape)\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self , p):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(1024, 512)\n",
    "        self.hidden2 = nn.Linear(512 , 256)\n",
    "        self.hidden3 =  nn.Linear(256 , 128)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.fc = nn.Linear(128, 6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.dropout(self.hidden(x)))\n",
    "        x = self.activation(self.dropout(self.hidden2(x)))\n",
    "        x = self.activation(self.dropout(self.hidden3(x)))\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_ds = TensorDataset(x_train, y_train)\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size , shuffle=True)\n",
    "\n",
    "valid_ds = TensorDataset(x_valid, y_valid )\n",
    "valid_dl = DataLoader(valid_ds , batch_size= batch_size)\n",
    "\n",
    "test_ds = TensorDataset(x_test , y_test)\n",
    "test_dl = DataLoader(test_ds , batch_size=batch_size)\n",
    "\n",
    "test_ds2 = TensorDataset(x_test_sp , y_test_sp)\n",
    "test_dl2 = DataLoader(test_ds2 , batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrappedDataLoader():\n",
    "    def __init__(self, dl, func):\n",
    "        self.dl = dl\n",
    "        self.func = func\n",
    "        \n",
    "    def __len__(self): return len(self.dl)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        batches = iter(self.dl)\n",
    "        for b in batches: yield(self.func(*b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "def preprocess(x,y): return x.to(dev),y.to(dev)\n",
    "\n",
    "train_dl = WrappedDataLoader(train_dl, preprocess)\n",
    "valid_dl = WrappedDataLoader(valid_dl, preprocess)\n",
    "test_dl = WrappedDataLoader(test_dl , preprocess)\n",
    "test_dl2 = WrappedDataLoader(test_dl2 , preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def binary_accuracy(preds, y):\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.mean() #/ (len(correct))\n",
    "    return acc\n",
    "\n",
    "\n",
    "def fbeta_score(y_true, y_pred, beta, threshold, eps=1e-9):\n",
    "    beta2 = beta**2\n",
    "\n",
    "    y_pred = torch.ge(torch.sigmoid(y_pred).float(), threshold).float()\n",
    "    y_true = y_true.float()\n",
    "\n",
    "    true_positive = (y_pred * y_true).sum(dim=0)\n",
    "    precision = true_positive.div(y_pred.sum(dim=0).add(eps))\n",
    "    recall = true_positive.div(y_true.sum(dim=0).add(eps))\n",
    "    \n",
    "    return torch.mean(\n",
    "        (precision*recall).\n",
    "        div(precision.mul(beta2) + recall + eps).\n",
    "        mul(1 + beta2)) , torch.mean(precision) , torch.mean(recall)\n",
    "\n",
    "\n",
    "def f1_score(y_pred,y_true, threshold=0.5):\n",
    "    f1 , precision , recall = fbeta_score(y_true, y_pred, 1, threshold) \n",
    "    return f1 , precision , recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0  \n",
    "    epoch_f1 = 0 ; epoch_precision = 0 ; epoch_recall = 0\n",
    "    model.train()\n",
    "    ct = 0\n",
    "    for x, y in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(x)\n",
    "        loss = criterion(predictions, y)\n",
    "        acc = binary_accuracy(predictions, y)\n",
    "        f1 , precision , recall = f1_score(predictions , y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        epoch_f1 += f1.item() \n",
    "        epoch_precision += precision.item()  \n",
    "        epoch_recall += recall.item()\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator) , epoch_f1/len(iterator), epoch_precision/len(iterator), epoch_recall/len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0 \n",
    "    epoch_f1 = 0; epoch_precision = 0 ; epoch_recall = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x ,y  in iterator:\n",
    "\n",
    "            predictions = model(x)#.squeeze(1)\n",
    "            loss = criterion(predictions,y)\n",
    "            acc = binary_accuracy(predictions, y) ; f1 , precision , recall = f1_score(predictions , y)\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "            epoch_f1 += f1.item()   ; epoch_precision += precision.item()  ; epoch_recall += recall.item()\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator) , epoch_f1/len(iterator), epoch_precision/len(iterator), epoch_recall/len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will use weight to scale loss value . It will result in giving equal weight to each category irrespective of data imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_ratio = train_aspects.aspects.apply(pd.Series).merge(train_aspects , right_index = True , left_index = True)\\\n",
    ".drop([ 'aspects' , 'polarities'] ,axis = 1).melt(id_vars = ['text']).drop(['variable'] , axis = 1).dropna().value.value_counts(normalize = True).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FOOD</td>\n",
       "      <td>0.361111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RESTAURANT</td>\n",
       "      <td>0.270531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SERVICE</td>\n",
       "      <td>0.198671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AMBIENCE</td>\n",
       "      <td>0.117754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DRINKS</td>\n",
       "      <td>0.038043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LOCATION</td>\n",
       "      <td>0.013889</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        index     value\n",
       "0  FOOD        0.361111\n",
       "1  RESTAURANT  0.270531\n",
       "2  SERVICE     0.198671\n",
       "3  AMBIENCE    0.117754\n",
       "4  DRINKS      0.038043\n",
       "5  LOCATION    0.013889"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "        \n",
    "weight_list =  [1/df_data_ratio[df_data_ratio['index']==c]['value'].values[0]  for c in mlb.classes_]\n",
    "weights = torch.tensor( weight_list)\n",
    "weights =weights.to(dev)\n",
    "\n",
    "from torch import optim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply grid search on Learning Rate, Weight Decay , Dropout parameters save the model with best validation loss and validation f1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/swati.tiwari/nlp/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data 0.7798833684487776 0.21473451297391544 0.2819275476715781 0.23644619773734699\n",
      "valid data 0.8620581030845642 0.22622336695591608 0.25614674886067706 0.21116616825262705\n",
      "Parameters:  Dropout:  0.2 weight decay:  0.1  learning rate :  0.01\n",
      "train data 0.8516593683849681 0.33419464934955945 0.4514863660389727 0.3033361340110952\n",
      "valid data 0.848248134056727 0.28686009099086124 0.41156866649786633 0.2589774827162425\n",
      "Parameters:  Dropout:  0.2 weight decay:  0.1  learning rate :  0.01\n",
      "train data 0.8655733655799519 0.3972922902215611 0.4685949601910331 0.36998756568540225\n",
      "valid data 0.8607165614763895 0.3591578851143519 0.49420441687107086 0.32077595591545105\n",
      "Parameters:  Dropout:  0.2 weight decay:  0.1  learning rate :  0.01\n",
      "train data 0.8683174279603091 0.41430715132843365 0.489790984175422 0.37984616512602026\n",
      "valid data 0.8783538738886515 0.36711075405279797 0.49450937906901044 0.3139963895082474\n",
      "Parameters:  Dropout:  0.2 weight decay:  0.1  learning rate :  0.01\n",
      "train data 0.8557162772525441 0.3894987146962773 0.46263137324289844 0.35800904441963544\n",
      "valid data 0.8625710407892863 0.3931034604708354 0.49496108293533325 0.37464090685049695\n",
      "Parameters:  Dropout:  0.2 weight decay:  0.1  learning rate :  0.01\n",
      "train data 0.8692966780879281 0.46154808456247504 0.5943252295255661 0.41222996332428674\n",
      "valid data 0.8983191549777985 0.45508117973804474 0.588167279958725 0.4004019995530446\n",
      "Parameters:  Dropout:  0.2 weight decay:  0.1  learning rate :  0.005\n",
      "train data 0.8712013797326521 0.4501305439255454 0.5851309407841075 0.39874415370551025\n",
      "valid data 0.9004892905553182 0.4602902481953303 0.620465079943339 0.40123217304547626\n",
      "Parameters:  Dropout:  0.2 weight decay:  0.1  learning rate :  0.001\n",
      "train data 0.8797563910484314 0.5301225036382675 0.6704880121079358 0.4694105874408375\n",
      "valid data 0.9018308222293854 0.4920693387587865 0.6057413816452026 0.43892802298069\n",
      "Parameters:  Dropout:  0.2 weight decay:  0.05  learning rate :  0.001\n",
      "train data 0.8705557178367268 0.5423090417276729 0.6684595238078724 0.48399353704669257\n",
      "valid data 0.908775269985199 0.5151392469803492 0.6221395631631216 0.47141974171002704\n",
      "Parameters:  Dropout:  0.4 weight decay:  0.05  learning rate :  0.001\n"
     ]
    }
   ],
   "source": [
    "best_valid_f1 = -float('inf') ; best_valid_loss = float('inf')\n",
    "loss_func = nn.BCEWithLogitsLoss(weight=weights) \n",
    "loss_func = loss_func.to(dev)\n",
    "for drp in [0.2, 0.3,0.4,0.5,0.6]:\n",
    "    for wd in [0.1 , 0.05 , 0.01 , 0.005 , 0.001]:\n",
    "        for learning_rate in [1e-2 , 5e-3 , 1e-3]:\n",
    "            model = Model(drp); model.apply(init_weights)\n",
    "            model = model.to(dev)\n",
    "            optimizer = optim.Adam(model.parameters() , lr = learning_rate, weight_decay=wd) #[a+'_pred' for a in aspects]\n",
    "            model = model.to(dev)\n",
    "            epochs = 10\n",
    "            for epoch in range(1, epochs + 1):\n",
    "                train_loss , train_acc , train_f1 , train_precision , train_recall = train_model(model, train_dl, optimizer, loss_func)\n",
    "                valid_loss , valid_acc , valid_f1 , valid_precision , valid_recall  = validate_model(model, valid_dl, loss_func)\n",
    "                if (valid_loss < best_valid_loss) & (valid_f1 > best_valid_f1)  & (abs(train_f1- valid_f1) <= 0.05):\n",
    "                    best_valid_f1 = valid_f1 ; best_valid_loss = valid_loss\n",
    "                    print('train data' , train_acc , train_f1 , train_precision , train_recall)\n",
    "                    print('valid data' , valid_acc ,  valid_f1 , valid_precision , valid_recall)\n",
    "\n",
    "\n",
    "                    print(\"Parameters: \" ,'Dropout: ' ,  drp , 'weight decay: ' ,wd ,' learning rate : ' ,learning_rate )\n",
    "                    if os.path.isfile('results/multi_label_problem.pt'):\n",
    "                        os.remove('results/multi_label_problem.pt') \n",
    "                                           \n",
    "                    torch.save(model.state_dict(), 'results/multi_label_problem.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.7852152188618977,\n",
       " 0.908775269985199,\n",
       " 0.5151392469803492,\n",
       " 0.6221395631631216,\n",
       " 0.47141974171002704)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_func = nn.BCEWithLogitsLoss(weight=weights) \n",
    "loss_func = loss_func.to(dev)\n",
    "model = Model(0.5)\n",
    "model.load_state_dict(torch.load('results/multi_label_problem.pt'))\n",
    "model = model.to(dev)\n",
    "validate_model(model, valid_dl, loss_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate aspect prediction for validation dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/swati.tiwari/nlp/lib/python3.6/site-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "val_preds = []\n",
    "val_label = []\n",
    "with torch.no_grad():\n",
    "    for x ,y  in valid_dl:\n",
    "        predictions = model(x)\n",
    "        rounded_preds = torch.round(torch.sigmoid(predictions))  #torch.round\n",
    "        preds = rounded_preds.data.cpu().numpy()\n",
    "        val_preds.append(preds)\n",
    "        val_label.append(y.data.cpu().numpy())\n",
    "\n",
    "val_aspects['aspects_pred'] = pd.Series(mlb.inverse_transform(np.vstack(val_preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate aspect prediction for dutch test dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score 0.5371728544695706\n",
      "Accuracy score 0.8720577069096431\n",
      "Precision score 0.7801170234947093\n",
      "Recall score 0.4685074726379324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/swati.tiwari/nlp/lib/python3.6/site-packages/ipykernel_launcher.py:22: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "/data/swati.tiwari/nlp/lib/python3.6/site-packages/ipykernel_launcher.py:23: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "/data/swati.tiwari/nlp/lib/python3.6/site-packages/ipykernel_launcher.py:24: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "/data/swati.tiwari/nlp/lib/python3.6/site-packages/ipykernel_launcher.py:25: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    }
   ],
   "source": [
    "test_preds = []\n",
    "true_label = []\n",
    "with torch.no_grad():\n",
    "    for x ,y  in test_dl:\n",
    "        predictions = model(x)#.squeeze(1)\n",
    "        rounded_preds = torch.round(torch.sigmoid(predictions))  #torch.round\n",
    "        preds = rounded_preds.data.cpu().numpy()\n",
    "        test_preds.append(preds)\n",
    "        true_label.append(y.data.cpu().numpy())\n",
    "        \n",
    "du_multi_aspects['aspects_pred']  = pd.Series(mlb.inverse_transform(np.vstack(test_preds)))\n",
    "\n",
    "aspects = mlb.classes_.tolist()\n",
    "\"\"\"\n",
    "Merging prediction value with original test data and observe the metrics on overall level\n",
    "\"\"\"\n",
    "dutch_pred = pd.DataFrame(np.vstack(test_preds) ,index=du_multi_aspects.index , columns= [a+'_pred' for a in aspects])\n",
    "dutch_pred2 = pd.merge(du_multi_aspects, dutch_pred , left_index=True ,right_index = True)\n",
    "\n",
    "from sklearn.metrics import f1_score , confusion_matrix , accuracy_score , precision_score , recall_score , roc_auc_score\n",
    "\n",
    "print(\"F1 score\",f1_score(y_du , dutch_pred2[[a+'_pred' for a in aspects]].as_matrix() , average='macro' ))\n",
    "print(\"Accuracy score\" , np.mean(y_du == dutch_pred2[[a+'_pred' for a in aspects]].as_matrix()))\n",
    "print(\"Precision score\",precision_score(y_du , dutch_pred2[[a+'_pred' for a in aspects]].as_matrix() , average='macro' ))\n",
    "print(\"Recall score\",recall_score(y_du , dutch_pred2[[a+'_pred' for a in aspects]].as_matrix() , average='macro' ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate aspect prediction for spanish test dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score 0.5451493396566448\n",
      "Accuracy score 0.8865313653136532\n",
      "Precision score 0.7285837584595787\n",
      "Recall score 0.4719908182638046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/swati.tiwari/nlp/lib/python3.6/site-packages/ipykernel_launcher.py:23: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "/data/swati.tiwari/nlp/lib/python3.6/site-packages/ipykernel_launcher.py:24: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "/data/swati.tiwari/nlp/lib/python3.6/site-packages/ipykernel_launcher.py:25: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "/data/swati.tiwari/nlp/lib/python3.6/site-packages/ipykernel_launcher.py:26: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    }
   ],
   "source": [
    "test_preds = []\n",
    "true_label = []\n",
    "with torch.no_grad():\n",
    "    for x ,y  in test_dl2:\n",
    "        predictions = model(x)#.squeeze(1)\n",
    "        rounded_preds = torch.round(torch.sigmoid(predictions))  #torch.round\n",
    "        preds = rounded_preds.data.cpu().numpy()\n",
    "        test_preds.append(preds)\n",
    "        true_label.append(y.data.cpu().numpy())\n",
    "        \n",
    "spanish_multi_aspects['aspects_pred']  = pd.Series(mlb.inverse_transform(np.vstack(test_preds)))\n",
    "\n",
    "        \n",
    "aspects = mlb.classes_.tolist()\n",
    "\"\"\"\n",
    "Merging prediction value with original test data and observe the metrics on overall level\n",
    "\"\"\"\n",
    "spanish_pred = pd.DataFrame(np.vstack(test_preds) ,index=spanish_multi_aspects.index , columns= [a+'_pred' for a in aspects])\n",
    "spanish_pred2 = pd.merge(spanish_multi_aspects, spanish_pred , left_index=True ,right_index = True)\n",
    "\n",
    "from sklearn.metrics import f1_score , confusion_matrix , accuracy_score , precision_score , recall_score , roc_auc_score\n",
    "\n",
    "print(\"F1 score\",f1_score(y_spainish , spanish_pred2[[a+'_pred' for a in aspects]].as_matrix() , average='macro' ))\n",
    "print(\"Accuracy score\" , np.mean(y_spainish == spanish_pred2[[a+'_pred' for a in aspects]].as_matrix()))\n",
    "print(\"Precision score\",precision_score(y_spainish , spanish_pred2[[a+'_pred' for a in aspects]].as_matrix() , average='macro' ))\n",
    "print(\"Recall score\",recall_score(y_spainish , spanish_pred2[[a+'_pred' for a in aspects]].as_matrix() , average='macro' ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next step is sentiment classification. We will work with only correct Text X Aspect results to predict sentiment.  Hence will will create an extra column aspects_pred for validation and test datasets.  Then we will filter out the correct  Text X Aspect combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/swati.tiwari/nlp/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "val_aspects['aspects_pred'] = val_aspects['aspects_pred'].apply(lambda x: list(x))\n",
    "du_multi_aspects['aspects_pred'] = du_multi_aspects['aspects_pred'].apply(lambda x: list(x))\n",
    "spanish_multi_aspects['aspects_pred'] = spanish_multi_aspects['aspects_pred'].apply(lambda x: list(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ungrp(train_df):\n",
    "    asp_df = train_df.aspects.apply(pd.Series).merge(train_df , right_index = True , left_index = True)\\\n",
    "    .drop(['aspects' , 'polarities' , 'aspects_pred'] ,axis = 1)\\\n",
    "    .melt(id_vars = ['text']).drop(['variable'] , axis = 1).dropna()\n",
    "\n",
    "    polarity_df = train_df.polarities.apply(pd.Series).merge(train_df , right_index = True , left_index = True)\\\n",
    "    .drop(['aspects' , 'polarities' , 'aspects_pred'] ,axis = 1)\\\n",
    "    .melt(id_vars = ['text']).drop(['variable'] , axis = 1).dropna()\n",
    "\n",
    "    train_df_ungrp = pd.merge(asp_df , polarity_df['value'] , left_index = True , right_index = True ,suffixes=('_aspects' , '_polarities'))\n",
    "    train_df_ungrp.rename(columns={'value_aspects' : 'aspects' , 'value_polarities':'polarities'} , inplace=True)\n",
    "\n",
    "    train_df_ungrp2 = pd.merge(train_df_ungrp , train_df[[ 'text' ,'aspects_pred']] , on ='text')\n",
    "    return train_df_ungrp2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_aspects_ungrp  = ungrp(val_aspects)\n",
    "du_aspects_ungrp = ungrp(du_multi_aspects)\n",
    "sp_aspects_ungrp= ungrp(spanish_multi_aspects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1629, 4), (2321, 4), (417, 4))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "du_aspects_ungrp.shape , sp_aspects_ungrp.shape , val_aspects_ungrp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_ind(x):\n",
    "    asp = x['aspects_pred']\n",
    "    if x['aspects'] in asp:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_aspects_ungrp['ind'] = val_aspects_ungrp.apply(lambda x: check_ind(x) , axis = 1)\n",
    "du_aspects_ungrp['ind'] = du_aspects_ungrp.apply(lambda x: check_ind(x) , axis = 1)\n",
    "sp_aspects_ungrp['ind'] = sp_aspects_ungrp.apply(lambda x: check_ind(x) , axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6930455635491607, 0.5960712093308779, 0.6604911676001723)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_aspects_ungrp.ind.mean() , du_aspects_ungrp.ind.mean() , sp_aspects_ungrp.ind.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ungrp2(train_df):\n",
    "    asp_df = train_df.aspects.apply(pd.Series).merge(train_df , right_index = True , left_index = True)\\\n",
    "    .drop(['aspects' , 'polarities'] ,axis = 1)\\\n",
    "    .melt(id_vars = ['text']).drop(['variable'] , axis = 1).dropna()\n",
    "\n",
    "    polarity_df = train_df.polarities.apply(pd.Series).merge(train_df , right_index = True , left_index = True)\\\n",
    "    .drop(['aspects' , 'polarities'] ,axis = 1)\\\n",
    "    .melt(id_vars = ['text']).drop(['variable'] , axis = 1).dropna()\n",
    "\n",
    "    train_df_ungrp = pd.merge(asp_df , polarity_df['value'] , left_index = True , right_index = True ,suffixes=('_aspects' , '_polarities'))\n",
    "    train_df_ungrp.rename(columns={'value_aspects' : 'aspects' , 'value_polarities':'polarities'} , inplace=True)\n",
    "\n",
    "    return train_df_ungrp\n",
    "train_aspects_ungrp = ungrp2(train_aspects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>aspects</th>\n",
       "      <th>polarities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The food was very good, a great deal, and the place its self was great.</td>\n",
       "      <td>AMBIENCE</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Terrible would be a compliment!</td>\n",
       "      <td>RESTAURANT</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                      text  \\\n",
       "0  The food was very good, a great deal, and the place its self was great.   \n",
       "1  Terrible would be a compliment!                                           \n",
       "\n",
       "      aspects polarities  \n",
       "0  AMBIENCE    positive   \n",
       "1  RESTAURANT  negative   "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_aspects_ungrp.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see for the given text , we have been able to predict only 2 out 5 aspects correctly. We will filter out the text X aspects combination which our Multi Label Aspect is not able to predict correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>aspects</th>\n",
       "      <th>polarities</th>\n",
       "      <th>aspects_pred</th>\n",
       "      <th>ind</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Everything was wonderful; food, drinks, staff, mileau.</td>\n",
       "      <td>AMBIENCE</td>\n",
       "      <td>positive</td>\n",
       "      <td>[FOOD, SERVICE]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Everything was wonderful; food, drinks, staff, mileau.</td>\n",
       "      <td>SERVICE</td>\n",
       "      <td>positive</td>\n",
       "      <td>[FOOD, SERVICE]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Everything was wonderful; food, drinks, staff, mileau.</td>\n",
       "      <td>FOOD</td>\n",
       "      <td>positive</td>\n",
       "      <td>[FOOD, SERVICE]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Everything was wonderful; food, drinks, staff, mileau.</td>\n",
       "      <td>DRINKS</td>\n",
       "      <td>positive</td>\n",
       "      <td>[FOOD, SERVICE]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Everything was wonderful; food, drinks, staff, mileau.</td>\n",
       "      <td>RESTAURANT</td>\n",
       "      <td>positive</td>\n",
       "      <td>[FOOD, SERVICE]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text     aspects  \\\n",
       "20  Everything was wonderful; food, drinks, staff, mileau.  AMBIENCE     \n",
       "21  Everything was wonderful; food, drinks, staff, mileau.  SERVICE      \n",
       "22  Everything was wonderful; food, drinks, staff, mileau.  FOOD         \n",
       "23  Everything was wonderful; food, drinks, staff, mileau.  DRINKS       \n",
       "24  Everything was wonderful; food, drinks, staff, mileau.  RESTAURANT   \n",
       "\n",
       "   polarities     aspects_pred  ind  \n",
       "20  positive   [FOOD, SERVICE]  0    \n",
       "21  positive   [FOOD, SERVICE]  1    \n",
       "22  positive   [FOOD, SERVICE]  1    \n",
       "23  positive   [FOOD, SERVICE]  0    \n",
       "24  positive   [FOOD, SERVICE]  0    "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_aspects_ungrp[val_aspects_ungrp['text']=='Everything was wonderful; food, drinks, staff, mileau.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_aspects_ungrp2 = val_aspects_ungrp[val_aspects_ungrp['ind']==1]\n",
    "du_aspects_ungrp2 = du_aspects_ungrp[du_aspects_ungrp['ind']==1]\n",
    "sp_aspects_ungrp2 = sp_aspects_ungrp[sp_aspects_ungrp['ind']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_aspects_ungrp.to_csv('../data/Train_english_restaurants_ungrp.csv' , index = False)\n",
    "val_aspects_ungrp2[['text', 'aspects', 'polarities']].to_csv('../data/Valid_english_restaurants_ungrp.csv' , index = False)\n",
    "du_aspects_ungrp2[['text', 'aspects', 'polarities']].to_csv('../data/Dutch_restaurants_ungrp.csv' , index = False)\n",
    "sp_aspects_ungrp2[['text', 'aspects', 'polarities']].to_csv('../data/Spanish_english_restaurants_ungrp.csv' , index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
