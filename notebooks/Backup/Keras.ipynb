{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import collections\n",
    "import pickle\n",
    "import re\n",
    "import random\n",
    "import sys\n",
    "import os \n",
    "import time\n",
    "\n",
    "import gensim\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from gensim.models.fasttext import FastText\n",
    "from gensim.models import word2vec\n",
    "\n",
    "from sklearn.model_selection import train_test_split,cross_val_score, cross_val_predict, KFold, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix,precision_recall_fscore_support\n",
    "from sklearn import svm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from numpy import cumsum\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras import initializers\n",
    "from keras.models import Sequential,Model,load_model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dropout, Activation, Flatten, \\\n",
    "    Embedding, Convolution1D, MaxPooling1D, AveragePooling1D, \\\n",
    "    Input, Dense, merge, Add,TimeDistributed, Bidirectional,SpatialDropout1D, \\\n",
    "    Concatenate, concatenate, multiply, Lambda, dot, Layer\n",
    "from keras.layers.recurrent import LSTM, GRU, SimpleRNN\n",
    "from keras.regularizers import l2, l1_l2\n",
    "from keras.constraints import maxnorm\n",
    "from keras import callbacks\n",
    "from keras.utils import generic_utils,plot_model\n",
    "from keras.optimizers import Adadelta,Adam\n",
    "from keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "# from keras.engine.topology import Layer\n",
    "\n",
    "\n",
    "import matplotlib.image  as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import gensim.downloader as gensimapi\n",
    "from collections import Counter, defaultdict\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Cross Lingual Aspect Based Sentiment Analysis- Part 1.ipynb'\r\n",
      "'Cross Lingual Aspect Based Sentiment Analysis- Part 2.ipynb'\r\n",
      " keras2.ipynb\r\n",
      " Keras.ipynb\r\n",
      "'Multilingual  semeval  dataset creation.ipynb'\r\n",
      " results\r\n",
      "'Tatoeba Similarity Search.ipynb'\r\n",
      " Twitter_Data.ipynb\r\n",
      " utils\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_folder = \"../../../corpus/new/preprocess_from_isuru/\"\n",
    "# lankadeepa_data_path = data_folder + 'lankadeepa_tagged_comments.csv'\n",
    "# gossip_lanka_data_path = data_folder + 'gossip_lanka_tagged_comments.csv'\n",
    "\n",
    "# lankadeepa_data = pd.read_csv(lankadeepa_data_path)[:9059]\n",
    "# gossipLanka_data = pd.read_csv(gossip_lanka_data_path)\n",
    "# gossipLanka_data = gossipLanka_data.drop(columns=['Unnamed: 3'])\n",
    "\n",
    "# sinhala_data = pd.concat([lankadeepa_data,gossipLanka_data], ignore_index=True)\n",
    "# sinhala_data.to_csv(\"../data/sinhala_comments.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_aspects_ungrp = pd.read_csv('../data/Train_english_restaurants_ungrp.csv')\n",
    "val_aspects_ungrp2 = pd.read_csv('../data/Valid_english_restaurants_ungrp.csv')\n",
    "du_aspects_ungrp2  = pd.read_csv('../data/Dutch_restaurants_ungrp.csv')\n",
    "sp_aspects_ungrp2  = pd.read_csv('../data/Spanish_english_restaurants_ungrp.csv')\n",
    "\n",
    "sinhala_comments  = pd.read_csv('../data/sinhala_comments.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "sinhala_comments = sinhala_comments.loc[sinhala_comments['label'].isin([2, 3, 4])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2 7665]\n",
      " [   3 2403]\n",
      " [   4 3080]]\n"
     ]
    }
   ],
   "source": [
    "(unique, counts) = np.unique(sinhala_comments[\"label\"].values, return_counts = True)\n",
    "frequencies = np.asarray((unique, counts)).T\n",
    "print(frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_aspects_ungrp[['text']].to_csv('../data/processed/en_resturant.csv' , header = None , index = None , mode = 'w')\n",
    "val_aspects_ungrp2[['text']].to_csv('../data/processed/en_val.csv' , header = None , index = None , mode = 'w')\n",
    "du_aspects_ungrp2[['text']].to_csv('../data/processed/nl_resturant.csv' , header = None , index = None , mode = 'w')\n",
    "sp_aspects_ungrp2[['text']].to_csv('../data/processed/es_resturant.csv' , header = None , index = None , mode = 'w')\n",
    "\n",
    "sinhala_comments[['comment']].to_csv('../data/processed/sinhala_comments.csv' , header = None , index = None , mode = 'w')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "LASER_PATH = \"/home/lahiru/Projects/FYP/LASER/LASER\"\n",
    "sys.path.insert(0, LASER_PATH + '/source/lib')\n",
    "sys.path.insert(1, LASER_PATH + '/source')\n",
    "\n",
    "DATA_PATH = Path(\"../data/processed/\")\n",
    "CACHE_PATH = Path(\"../cache2/\")\n",
    "CACHE_PATH.mkdir(exist_ok=True)\n",
    "MODEL_PATH = Path(LASER_PATH + \"/models\")\n",
    "\n",
    "os.environ[\"LASER\"] = LASER_PATH \n",
    "SPACE_NORMALIZER = re.compile(\"\\s+\")\n",
    "Batch = namedtuple('Batch', 'srcs tokens lengths')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from indexing import IndexLoad, IndexTextOpen, IndexTextQuery, IndexSearchKNN, IndexCreate, IndexSearchMultiple\n",
    "from embed import SentenceEncoder, EncodeLoad, EncodeFile\n",
    "from text_processing import Token, BPEfastApply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Tokenizer: en_resturant.csv exists already\n",
      " - Tokenizer: nl_resturant.csv exists already\n",
      " - Tokenizer: es_resturant.csv exists already\n",
      " - Tokenizer: en_val.csv exists already\n"
     ]
    }
   ],
   "source": [
    "encoder = SentenceEncoder(\n",
    "    str(MODEL_PATH / \"bilstm.93langs.2018-12-26.pt\"),\n",
    "    max_sentences=None,\n",
    "    max_tokens=10000,\n",
    "    cpu=False)\n",
    "#original : cpu = False\n",
    "\n",
    "bpe_codes = str(MODEL_PATH / \"93langs.fcodes\")\n",
    "\n",
    "for lang in (\"en\" ,\"nl\", 'es'): \n",
    "    Token(\n",
    "        str(DATA_PATH / f\"{lang}_resturant.csv\"), ##english_resturant.txt\n",
    "        str(CACHE_PATH / f\"{lang}_resturant.csv\"),\n",
    "        lang=lang,\n",
    "        romanize=False,\n",
    "        lower_case=True, gzip=False,\n",
    "        verbose=True)\n",
    "    BPEfastApply(\n",
    "        str(CACHE_PATH / f\"{lang}_resturant.csv\"),\n",
    "        str(CACHE_PATH / f\"{lang}_resturant.bpe\"),\n",
    "        bpe_codes,\n",
    "        verbose=True, over_write=True)\n",
    "    EncodeFile(\n",
    "        encoder,\n",
    "        str(CACHE_PATH / f\"{lang}_resturant.bpe\"),\n",
    "        str(CACHE_PATH / f\"{lang}_resturant.enc\"),\n",
    "        verbose=True, over_write=True)    \n",
    "    \n",
    "    \n",
    "Token(\n",
    "    str(DATA_PATH / f\"en_val.csv\"), ##english_resturant.txt\n",
    "    str(CACHE_PATH / f\"en_val.csv\"),\n",
    "    lang=lang,\n",
    "    romanize=False,\n",
    "    lower_case=True, gzip=False,\n",
    "    verbose=True)\n",
    "BPEfastApply(\n",
    "    str(CACHE_PATH / f\"en_val.csv\"),\n",
    "    str(CACHE_PATH / f\"en_val.bpe\"),\n",
    "    bpe_codes,\n",
    "    verbose=True, over_write=True)\n",
    "EncodeFile(\n",
    "    encoder,\n",
    "    str(CACHE_PATH / f\"en_val.bpe\"),\n",
    "    str(CACHE_PATH / f\"en_val.enc\"),\n",
    "    verbose=True, over_write=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Tokenizer: sinhala_comments.csv exists already\n"
     ]
    }
   ],
   "source": [
    "Token(\n",
    "    str(DATA_PATH / f\"sinhala_comments.csv\"), ##english_resturant.txt\n",
    "    str(CACHE_PATH / f\"sinhala_comments.csv\"),\n",
    "    lang=\"sin\",\n",
    "    romanize=False,\n",
    "    lower_case=True, gzip=False,\n",
    "    verbose=True)\n",
    "BPEfastApply(\n",
    "    str(CACHE_PATH / f\"sinhala_comments.csv\"),\n",
    "    str(CACHE_PATH / f\"sinhala_comments.bpe\"),\n",
    "    bpe_codes,\n",
    "    verbose=True, over_write=True)\n",
    "EncodeFile(\n",
    "    encoder,\n",
    "    str(CACHE_PATH / f\"sinhala_comments.bpe\"),\n",
    "    str(CACHE_PATH / f\"sinhala_comments.enc\"),\n",
    "    verbose=True, over_write=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "LASER_PATH = \"/home/lahiru/Projects/FYP/LASER/LASER\"\n",
    "sys.path.insert(0, LASER_PATH + '/source/lib')\n",
    "sys.path.insert(1, LASER_PATH + '/source')\n",
    "\n",
    "DATA_PATH = Path(\"../data/processed/\")\n",
    "CACHE_PATH = Path(\"../cache2/\")\n",
    "CACHE_PATH.mkdir(exist_ok=True)\n",
    "MODEL_PATH = Path(LASER_PATH + \"/models\")\n",
    "\n",
    "os.environ[\"LASER\"] = LASER_PATH \n",
    "SPACE_NORMALIZER = re.compile(\"\\s+\")\n",
    "Batch = namedtuple('Batch', 'srcs tokens lengths')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - embedding: ../cache2/en_resturant.enc 1656 examples of dim 1024\n",
      " - creating FAISS index\n",
      " - embedding: ../cache2/en_val.enc 283 examples of dim 1024\n",
      " - creating FAISS index\n",
      " - embedding: ../cache2/nl_resturant.enc 960 examples of dim 1024\n",
      " - creating FAISS index\n",
      " - embedding: ../cache2/es_resturant.enc 1416 examples of dim 1024\n",
      " - creating FAISS index\n",
      " - embedding: ../cache2/sinhala_comments.enc 13148 examples of dim 1024\n",
      " - creating FAISS index\n"
     ]
    }
   ],
   "source": [
    "train_en, index_tr_en = IndexCreate(\n",
    "    str(CACHE_PATH / \"en_resturant.enc\"), 'FlatL2', verbose=True, save_index=False)\n",
    "val_en, index_val_en = IndexCreate(\n",
    "    str(CACHE_PATH / \"en_val.enc\"), 'FlatL2', verbose=True, save_index=False)\n",
    "\n",
    "data_du, index_du = IndexCreate(\n",
    "    str(CACHE_PATH / \"nl_resturant.enc\"), 'FlatL2', verbose=True, save_index=False)\n",
    "data_spanish, index_spanish = IndexCreate(\n",
    "    str(CACHE_PATH / \"es_resturant.enc\"), 'FlatL2', verbose=True, save_index=False)\n",
    "\n",
    "data_sinhala, index_sinhala = IndexCreate(\n",
    "    str(CACHE_PATH / \"sinhala_comments.enc\"), 'FlatL2', verbose=True, save_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_target(x):\n",
    "    if x=='positive':\n",
    "        return 2\n",
    "    elif x =='negative':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0 \n",
    "    \n",
    "def change_target_sinala(x):\n",
    "    if x== 4:\n",
    "        return 2\n",
    "    elif x == 2:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0 \n",
    "\n",
    "train_aspects_ungrp['polarities'] = train_aspects_ungrp['polarities'].apply(lambda x: change_target(x))\n",
    "val_aspects_ungrp2['polarities'] = val_aspects_ungrp2['polarities'].apply(lambda x: change_target(x))\n",
    "sp_aspects_ungrp2['polarities'] = sp_aspects_ungrp2['polarities'].apply(lambda x: change_target(x))\n",
    "du_aspects_ungrp2['polarities'] = du_aspects_ungrp2['polarities'].apply(lambda x: change_target(x))\n",
    "\n",
    "sinhala_comments['label'] = sinhala_comments['label'].apply(lambda x: change_target_sinala(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1656,), (283,), (960,), (1416,), (13148,))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "tr_eng =  train_aspects_ungrp['polarities'].values\n",
    "val_eng =val_aspects_ungrp2['polarities'].values  \n",
    "y_du  = du_aspects_ungrp2['polarities'].values\n",
    "y_spainish  = sp_aspects_ungrp2['polarities'].values\n",
    "\n",
    "y_sinhala  = sinhala_comments['label'].values\n",
    "tr_eng.shape , val_eng.shape , y_du.shape , y_spainish.shape, y_sinhala.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0 2403]\n",
      " [   1 7665]\n",
      " [   2 3080]]\n"
     ]
    }
   ],
   "source": [
    "(unique, counts) = np.unique(y_sinhala, return_counts = True)\n",
    "frequencies = np.asarray((unique, counts)).T\n",
    "print(frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_eng = pd.get_dummies(tr_eng).values\n",
    "val_eng = pd.get_dummies(val_eng).values\n",
    "y_du = pd.get_dummies(y_du).values\n",
    "y_spainish = pd.get_dummies(y_spainish).values\n",
    "y_sinhala = pd.get_dummies(y_sinhala).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1], dtype=uint8)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_eng[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "std_scale = StandardScaler().fit(train_en)\n",
    "train_std = std_scale.transform(train_en) \n",
    "val_std = std_scale.transform(val_en)\n",
    "dutch_std = std_scale.transform(data_du)\n",
    "spanish_std = std_scale.transform(data_spanish)\n",
    "sinhala_std = std_scale.transform(data_sinhala)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1656, 1024)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_std.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps = 64\n",
    "features = int(len(train_std[0]))//64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_std = train_std.reshape(len(train_std), time_steps, features)\n",
    "sinhala_std = sinhala_std.reshape(len(sinhala_std), time_steps, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(sinhala_std, y_sinhala, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1656, 64, 16)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_std.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN_model():\n",
    "    main_input = Input(shape=(time_steps,features ), dtype='float', name='main_input')\n",
    "#     embedding  = Embedding(MAX_FEATURES, EMBEDDING_SIZE\n",
    "#               , input_length=MAX_LEN,\n",
    "#               name='embedding' ,trainable=False)(main_input)\n",
    "\n",
    "#     embedding = Dropout(DROPOUT_VALUE_1)(main_input)\n",
    "\n",
    "    x = RNN(HIDDEN_DIMS)(main_input)\n",
    "\n",
    "    x = Dense(HIDDEN_DIMS, activation='relu', init='he_normal', \n",
    "              W_constraint = maxnorm(3), b_constraint=maxnorm(3),\n",
    "              name='mlp')(x)\n",
    "\n",
    "    x = Dropout(DROPOUT_VALUE_2, name='drop')(x)\n",
    "\n",
    "    output = Dense(3, init='he_normal',\n",
    "                   activation='softmax', name='output')(x)\n",
    "\n",
    "    model = Model(input=main_input, output=output ,name=\"RNN_model\")\n",
    "\n",
    "    model.compile(loss={'output':'categorical_crossentropy'},\n",
    "              optimizer=Adadelta(lr=0.95, epsilon=1e-06),\n",
    "              metrics=[\"accuracy\",\n",
    "                       keras.metrics.Precision(),\n",
    "                        keras.metrics.Recall(),\n",
    "                       f1])\n",
    "    \n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "# def simple_model():\n",
    "#     model = tf.keras.Sequential()\n",
    "#     model.add(tf.keras.layers.Dense(512))\n",
    "#     model.add(tf.keras.layers.Dense(256))\n",
    "#     model.add(tf.keras.layers.Dense(3))\n",
    "#     model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "#     # This builds the model for the first time:\n",
    "    \n",
    "#     return model\n",
    "\n",
    "# def simple_model2():\n",
    "    \n",
    "#     model = Sequential()\n",
    "#     model.add(LSTM(32, input_shape=(1024, 1024)))\n",
    "#     model.add(Dense(3))\n",
    "#     print(model.summary())\n",
    "#     model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "#     return model\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_Model(model,X_train, y_train, validation_data , cross_validation = False):\n",
    "    print('Training and Testing...')\n",
    "    es = EarlyStopping(monitor='val_f1', mode='max', verbose=1, patience=5)\n",
    "    checkpoint = ModelCheckpoint(model_save_path, monitor='val_f1', verbose=1, save_best_only=True, mode='max')\n",
    "    callbacks_list = [checkpoint,es]\n",
    "    \n",
    "    if (cross_validation):\n",
    "        callbacks_list = [es]\n",
    "        \n",
    "    his = model.fit(X_train, y_train,validation_data = validation_data , epochs=NB_EPOCHS, batch_size=BATCH_SIZE, verbose=1)\n",
    "    #his = model.fit(X_train, y_train,validation_data = validation_data , epochs=NB_EPOCHS, batch_size=BATCH_SIZE, callbacks=callbacks_list, verbose=1)\n",
    "    return model,his"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_FEATURES = 300\n",
    "VERBOSITY = 1\n",
    "VALIDATION_SPLIT = 0.1\n",
    "NB_EPOCHS = 50\n",
    "FOLDS = 10\n",
    "EMBEDDING_SIZE = 300\n",
    "\n",
    "BATCH_SIZE = 32 # 64, 128\n",
    "NB_FILTERS = 200 #200\n",
    "FILTER_LENGTH = 4 # test with 2,3,4,5\n",
    "# HIDDEN_DIMS = NB_FILTERS * 2\n",
    "HIDDEN_DIMS = 32\n",
    "MAX_LEN = 210 #test with other values(only this value work for now)\n",
    "DROPOUT_VALUE_1 = 0.5 #0.8 #0.3\n",
    "DROPOUT_VALUE_2 = 0.5\n",
    "L2_REG= 0.01\n",
    "\n",
    "RNN = LSTM\n",
    "\n",
    "model_save_path = \"../trained_models/RNN1.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/lahiru/anaconda3/envs/LASER-tf-1/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /home/lahiru/anaconda3/envs/LASER-tf-1/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3172: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"RNN_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "main_input (InputLayer)      (None, 64, 16)            0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 32)                6272      \n",
      "_________________________________________________________________\n",
      "mlp (Dense)                  (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "drop (Dropout)               (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 7,427\n",
      "Trainable params: 7,427\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lahiru/anaconda3/envs/LASER-tf-1/lib/python3.6/site-packages/ipykernel_launcher.py:13: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(32, activation=\"relu\", name=\"mlp\", kernel_initializer=\"he_normal\", kernel_constraint=<keras.con..., bias_constraint=<keras.con...)`\n",
      "  del sys.path[0]\n",
      "/home/lahiru/anaconda3/envs/LASER-tf-1/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(3, activation=\"softmax\", name=\"output\", kernel_initializer=\"he_normal\")`\n",
      "/home/lahiru/anaconda3/envs/LASER-tf-1/lib/python3.6/site-packages/ipykernel_launcher.py:20: UserWarning: Update your `Model` call to the Keras 2 API: `Model(name=\"RNN_model\", inputs=Tensor(\"ma..., outputs=Tensor(\"ou...)`\n"
     ]
    }
   ],
   "source": [
    "model = RNN_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and Testing...\n",
      "Train on 1656 samples, validate on 2630 samples\n",
      "Epoch 1/50\n",
      "1656/1656 [==============================] - 3s 2ms/step - loss: 0.8678 - accuracy: 0.6141 - precision_4: 0.6831 - recall_4: 0.4022 - f1: 0.4726 - val_loss: 1.1203 - val_accuracy: 0.3567 - val_precision_4: 0.3416 - val_recall_4: 0.2141 - val_f1: 0.2642\n",
      "Epoch 2/50\n",
      "1656/1656 [==============================] - 2s 1ms/step - loss: 0.7436 - accuracy: 0.6727 - precision_4: 0.7071 - recall_4: 0.5990 - f1: 0.6465 - val_loss: 1.1127 - val_accuracy: 0.4681 - val_precision_4: 0.4768 - val_recall_4: 0.3821 - val_f1: 0.4244\n",
      "Epoch 3/50\n",
      "1656/1656 [==============================] - 1s 707us/step - loss: 0.7084 - accuracy: 0.7047 - precision_4: 0.7283 - recall_4: 0.6377 - f1: 0.6805 - val_loss: 1.4039 - val_accuracy: 0.2783 - val_precision_4: 0.2698 - val_recall_4: 0.2490 - val_f1: 0.2612\n",
      "Epoch 4/50\n",
      "1656/1656 [==============================] - 1s 799us/step - loss: 0.6780 - accuracy: 0.7156 - precision_4: 0.7397 - recall_4: 0.6606 - f1: 0.6969 - val_loss: 1.2401 - val_accuracy: 0.3726 - val_precision_4: 0.3669 - val_recall_4: 0.3232 - val_f1: 0.3414\n",
      "Epoch 5/50\n",
      "1656/1656 [==============================] - 1s 872us/step - loss: 0.6481 - accuracy: 0.7264 - precision_4: 0.7480 - recall_4: 0.6757 - f1: 0.7094 - val_loss: 1.2187 - val_accuracy: 0.4331 - val_precision_4: 0.4320 - val_recall_4: 0.3867 - val_f1: 0.4054\n",
      "Epoch 6/50\n",
      "1656/1656 [==============================] - 1s 733us/step - loss: 0.6186 - accuracy: 0.7385 - precision_4: 0.7646 - recall_4: 0.6944 - f1: 0.7277 - val_loss: 1.3222 - val_accuracy: 0.3673 - val_precision_4: 0.3659 - val_recall_4: 0.3300 - val_f1: 0.3449\n",
      "Epoch 7/50\n",
      "1656/1656 [==============================] - 1s 782us/step - loss: 0.5908 - accuracy: 0.7548 - precision_4: 0.7805 - recall_4: 0.7150 - f1: 0.7462 - val_loss: 1.3819 - val_accuracy: 0.4053 - val_precision_4: 0.4015 - val_recall_4: 0.3719 - val_f1: 0.3855\n",
      "Epoch 8/50\n",
      "1656/1656 [==============================] - 1s 780us/step - loss: 0.5562 - accuracy: 0.7736 - precision_4: 0.8009 - recall_4: 0.7313 - f1: 0.7635 - val_loss: 1.3393 - val_accuracy: 0.4156 - val_precision_4: 0.4160 - val_recall_4: 0.3726 - val_f1: 0.3925\n",
      "Epoch 9/50\n",
      "1656/1656 [==============================] - 1s 738us/step - loss: 0.5508 - accuracy: 0.7826 - precision_4: 0.8039 - recall_4: 0.7403 - f1: 0.7705 - val_loss: 1.5599 - val_accuracy: 0.3882 - val_precision_4: 0.3883 - val_recall_4: 0.3597 - val_f1: 0.3713\n",
      "Epoch 10/50\n",
      "1656/1656 [==============================] - 1s 782us/step - loss: 0.5087 - accuracy: 0.7983 - precision_4: 0.8244 - recall_4: 0.7627 - f1: 0.7915 - val_loss: 1.3589 - val_accuracy: 0.4563 - val_precision_4: 0.4578 - val_recall_4: 0.4209 - val_f1: 0.4372\n",
      "Epoch 11/50\n",
      "1656/1656 [==============================] - 1s 805us/step - loss: 0.4728 - accuracy: 0.8056 - precision_4: 0.8372 - recall_4: 0.7699 - f1: 0.8006 - val_loss: 1.5294 - val_accuracy: 0.4163 - val_precision_4: 0.4168 - val_recall_4: 0.3894 - val_f1: 0.4025\n",
      "Epoch 12/50\n",
      "1656/1656 [==============================] - 1s 823us/step - loss: 0.4490 - accuracy: 0.8188 - precision_4: 0.8458 - recall_4: 0.7880 - f1: 0.8159 - val_loss: 1.6215 - val_accuracy: 0.3992 - val_precision_4: 0.3991 - val_recall_4: 0.3783 - val_f1: 0.3860\n",
      "Epoch 13/50\n",
      "1656/1656 [==============================] - 1s 780us/step - loss: 0.4164 - accuracy: 0.8364 - precision_4: 0.8585 - recall_4: 0.8025 - f1: 0.8294 - val_loss: 1.7098 - val_accuracy: 0.4110 - val_precision_4: 0.4152 - val_recall_4: 0.3928 - val_f1: 0.4014\n",
      "Epoch 14/50\n",
      "1656/1656 [==============================] - 1s 829us/step - loss: 0.3861 - accuracy: 0.8442 - precision_4: 0.8654 - recall_4: 0.8194 - f1: 0.8417 - val_loss: 2.3017 - val_accuracy: 0.3445 - val_precision_4: 0.3442 - val_recall_4: 0.3335 - val_f1: 0.3370\n",
      "Epoch 15/50\n",
      "1656/1656 [==============================] - 1s 767us/step - loss: 0.3716 - accuracy: 0.8490 - precision_4: 0.8781 - recall_4: 0.8267 - f1: 0.8511 - val_loss: 2.2870 - val_accuracy: 0.3498 - val_precision_4: 0.3464 - val_recall_4: 0.3319 - val_f1: 0.3373\n",
      "Epoch 16/50\n",
      "1656/1656 [==============================] - 1s 771us/step - loss: 0.3320 - accuracy: 0.8647 - precision_4: 0.8878 - recall_4: 0.8454 - f1: 0.8656 - val_loss: 2.3605 - val_accuracy: 0.3563 - val_precision_4: 0.3559 - val_recall_4: 0.3437 - val_f1: 0.3477\n",
      "Epoch 17/50\n",
      "1656/1656 [==============================] - 1s 742us/step - loss: 0.3202 - accuracy: 0.8714 - precision_4: 0.8940 - recall_4: 0.8508 - f1: 0.8715 - val_loss: 2.1256 - val_accuracy: 0.4278 - val_precision_4: 0.4244 - val_recall_4: 0.4000 - val_f1: 0.4091\n",
      "Epoch 18/50\n",
      "1656/1656 [==============================] - 1s 739us/step - loss: 0.2912 - accuracy: 0.8889 - precision_4: 0.9085 - recall_4: 0.8696 - f1: 0.8885 - val_loss: 2.5366 - val_accuracy: 0.3814 - val_precision_4: 0.3809 - val_recall_4: 0.3677 - val_f1: 0.3718\n",
      "Epoch 19/50\n",
      "1656/1656 [==============================] - 1s 755us/step - loss: 0.2580 - accuracy: 0.9010 - precision_4: 0.9188 - recall_4: 0.8810 - f1: 0.8994 - val_loss: 3.1514 - val_accuracy: 0.3548 - val_precision_4: 0.3526 - val_recall_4: 0.3433 - val_f1: 0.3463\n",
      "Epoch 20/50\n",
      "1656/1656 [==============================] - 1s 783us/step - loss: 0.2655 - accuracy: 0.8998 - precision_4: 0.9147 - recall_4: 0.8810 - f1: 0.8974 - val_loss: 2.7520 - val_accuracy: 0.4046 - val_precision_4: 0.4034 - val_recall_4: 0.3859 - val_f1: 0.3921\n",
      "Epoch 21/50\n",
      "1656/1656 [==============================] - 1s 745us/step - loss: 0.2402 - accuracy: 0.9034 - precision_4: 0.9284 - recall_4: 0.8847 - f1: 0.9055 - val_loss: 2.8736 - val_accuracy: 0.3901 - val_precision_4: 0.3858 - val_recall_4: 0.3719 - val_f1: 0.3767\n",
      "Epoch 22/50\n",
      "1656/1656 [==============================] - 1s 751us/step - loss: 0.2234 - accuracy: 0.9088 - precision_4: 0.9264 - recall_4: 0.8895 - f1: 0.9074 - val_loss: 2.5809 - val_accuracy: 0.4471 - val_precision_4: 0.4440 - val_recall_4: 0.4236 - val_f1: 0.4307\n",
      "Epoch 23/50\n",
      "1656/1656 [==============================] - 1s 795us/step - loss: 0.2152 - accuracy: 0.9094 - precision_4: 0.9326 - recall_4: 0.8943 - f1: 0.9130 - val_loss: 2.7967 - val_accuracy: 0.4418 - val_precision_4: 0.4391 - val_recall_4: 0.4232 - val_f1: 0.4280\n",
      "Epoch 24/50\n",
      "1656/1656 [==============================] - 1s 752us/step - loss: 0.2045 - accuracy: 0.9130 - precision_4: 0.9285 - recall_4: 0.8943 - f1: 0.9111 - val_loss: 2.8837 - val_accuracy: 0.4529 - val_precision_4: 0.4553 - val_recall_4: 0.4414 - val_f1: 0.4455\n",
      "Epoch 25/50\n",
      "1656/1656 [==============================] - 1s 793us/step - loss: 0.2031 - accuracy: 0.9209 - precision_4: 0.9373 - recall_4: 0.9034 - f1: 0.9201 - val_loss: 3.1141 - val_accuracy: 0.4327 - val_precision_4: 0.4311 - val_recall_4: 0.4179 - val_f1: 0.4219\n",
      "Epoch 26/50\n",
      "1656/1656 [==============================] - 1s 770us/step - loss: 0.1778 - accuracy: 0.9324 - precision_4: 0.9463 - recall_4: 0.9149 - f1: 0.9302 - val_loss: 3.4218 - val_accuracy: 0.4095 - val_precision_4: 0.4074 - val_recall_4: 0.3954 - val_f1: 0.3989\n",
      "Epoch 27/50\n",
      "1656/1656 [==============================] - 1s 797us/step - loss: 0.1761 - accuracy: 0.9263 - precision_4: 0.9432 - recall_4: 0.9118 - f1: 0.9271 - val_loss: 3.3658 - val_accuracy: 0.4259 - val_precision_4: 0.4242 - val_recall_4: 0.4129 - val_f1: 0.4159\n",
      "Epoch 28/50\n",
      "1656/1656 [==============================] - 1s 810us/step - loss: 0.1568 - accuracy: 0.9324 - precision_4: 0.9536 - recall_4: 0.9179 - f1: 0.9349 - val_loss: 3.4995 - val_accuracy: 0.4186 - val_precision_4: 0.4186 - val_recall_4: 0.4027 - val_f1: 0.4079\n",
      "Epoch 29/50\n",
      "1656/1656 [==============================] - 1s 814us/step - loss: 0.1636 - accuracy: 0.9293 - precision_4: 0.9452 - recall_4: 0.9161 - f1: 0.9301 - val_loss: 3.1623 - val_accuracy: 0.4719 - val_precision_4: 0.4733 - val_recall_4: 0.4582 - val_f1: 0.4627\n",
      "Epoch 30/50\n",
      "1656/1656 [==============================] - 1s 769us/step - loss: 0.1582 - accuracy: 0.9306 - precision_4: 0.9500 - recall_4: 0.9185 - f1: 0.9338 - val_loss: 3.5253 - val_accuracy: 0.4251 - val_precision_4: 0.4237 - val_recall_4: 0.4057 - val_f1: 0.4121\n",
      "Epoch 31/50\n",
      "1656/1656 [==============================] - 1s 731us/step - loss: 0.1524 - accuracy: 0.9360 - precision_4: 0.9513 - recall_4: 0.9197 - f1: 0.9351 - val_loss: 4.0683 - val_accuracy: 0.4122 - val_precision_4: 0.4127 - val_recall_4: 0.4042 - val_f1: 0.4061\n",
      "Epoch 32/50\n",
      "1656/1656 [==============================] - 1s 730us/step - loss: 0.1552 - accuracy: 0.9354 - precision_4: 0.9538 - recall_4: 0.9215 - f1: 0.9369 - val_loss: 3.6196 - val_accuracy: 0.4179 - val_precision_4: 0.4183 - val_recall_4: 0.4061 - val_f1: 0.4096\n",
      "Epoch 33/50\n",
      "1656/1656 [==============================] - 1s 826us/step - loss: 0.1540 - accuracy: 0.9402 - precision_4: 0.9511 - recall_4: 0.9269 - f1: 0.9386 - val_loss: 4.3905 - val_accuracy: 0.3825 - val_precision_4: 0.3813 - val_recall_4: 0.3738 - val_f1: 0.3754\n",
      "Epoch 34/50\n",
      "1656/1656 [==============================] - 2s 989us/step - loss: 0.1488 - accuracy: 0.9336 - precision_4: 0.9527 - recall_4: 0.9251 - f1: 0.9385 - val_loss: 3.9637 - val_accuracy: 0.4221 - val_precision_4: 0.4244 - val_recall_4: 0.4152 - val_f1: 0.4171\n",
      "Epoch 35/50\n",
      "1656/1656 [==============================] - 2s 912us/step - loss: 0.1392 - accuracy: 0.9402 - precision_4: 0.9511 - recall_4: 0.9269 - f1: 0.9385 - val_loss: 4.6792 - val_accuracy: 0.3848 - val_precision_4: 0.3822 - val_recall_4: 0.3757 - val_f1: 0.3768\n",
      "Epoch 36/50\n",
      "1656/1656 [==============================] - 1s 770us/step - loss: 0.1423 - accuracy: 0.9342 - precision_4: 0.9447 - recall_4: 0.9179 - f1: 0.9310 - val_loss: 4.4848 - val_accuracy: 0.3954 - val_precision_4: 0.3978 - val_recall_4: 0.3905 - val_f1: 0.3918\n",
      "Epoch 37/50\n",
      "1656/1656 [==============================] - 1s 785us/step - loss: 0.1254 - accuracy: 0.9426 - precision_4: 0.9549 - recall_4: 0.9336 - f1: 0.9441 - val_loss: 4.3799 - val_accuracy: 0.4205 - val_precision_4: 0.4193 - val_recall_4: 0.4110 - val_f1: 0.4126\n",
      "Epoch 38/50\n",
      "1656/1656 [==============================] - 1s 872us/step - loss: 0.1318 - accuracy: 0.9402 - precision_4: 0.9506 - recall_4: 0.9306 - f1: 0.9401 - val_loss: 4.4943 - val_accuracy: 0.4114 - val_precision_4: 0.4128 - val_recall_4: 0.4057 - val_f1: 0.4067\n",
      "Epoch 39/50\n",
      "1656/1656 [==============================] - 2s 908us/step - loss: 0.1375 - accuracy: 0.9360 - precision_4: 0.9441 - recall_4: 0.9281 - f1: 0.9358 - val_loss: 4.9034 - val_accuracy: 0.3928 - val_precision_4: 0.3933 - val_recall_4: 0.3859 - val_f1: 0.3874\n",
      "Epoch 40/50\n",
      "1656/1656 [==============================] - 2s 1ms/step - loss: 0.1267 - accuracy: 0.9457 - precision_4: 0.9578 - recall_4: 0.9324 - f1: 0.9444 - val_loss: 5.2419 - val_accuracy: 0.3757 - val_precision_4: 0.3765 - val_recall_4: 0.3711 - val_f1: 0.3718\n",
      "Epoch 41/50\n",
      " 512/1656 [========>.....................] - ETA: 0s - loss: 0.1007 - accuracy: 0.9551 - precision_4: 0.9603 - recall_4: 0.9453 - f1: 0.9523"
     ]
    }
   ],
   "source": [
    "Train_Model(model,train_std, tr_eng,validation_data = (X_test, y_test), cross_validation = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(sinhala_std, y_sinhala, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC(decision_function_shape='ovo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(decision_function_shape='ovo')"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(train_std, tr_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.0000    0.0000    0.0000       453\n",
      "           1     0.7725    0.5251    0.6252      1571\n",
      "           2     0.3412    0.8795    0.4917       606\n",
      "\n",
      "    accuracy                         0.5163      2630\n",
      "   macro avg     0.3712    0.4682    0.3723      2630\n",
      "weighted avg     0.5401    0.5163    0.4868      2630\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lahiru/anaconda3/envs/LASER-tf-1/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "report_print = classification_report(y_test, prediction, digits=4)\n",
    "print(report_print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
