{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import collections\n",
    "import pickle\n",
    "import re\n",
    "import random\n",
    "import sys\n",
    "import os \n",
    "import time\n",
    "\n",
    "import gensim\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from gensim.models.fasttext import FastText\n",
    "from gensim.models import word2vec\n",
    "\n",
    "from sklearn.model_selection import train_test_split,cross_val_score, cross_val_predict, KFold, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix,precision_recall_fscore_support\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from numpy import cumsum\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras import initializers\n",
    "from keras.models import Sequential,Model,load_model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dropout, Activation, Flatten, \\\n",
    "    Embedding, Convolution1D, MaxPooling1D, AveragePooling1D, \\\n",
    "    Input, Dense, merge, Add,TimeDistributed, Bidirectional,SpatialDropout1D, \\\n",
    "    Concatenate, concatenate, multiply, Lambda, dot, Layer\n",
    "from keras.layers.recurrent import LSTM, GRU, SimpleRNN\n",
    "from keras.regularizers import l2, l1_l2\n",
    "from keras.constraints import maxnorm\n",
    "from keras import callbacks\n",
    "from keras.utils import generic_utils,plot_model\n",
    "from keras.optimizers import Adadelta,Adam\n",
    "from keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "# from keras.engine.topology import Layer\n",
    "\n",
    "\n",
    "import matplotlib.image  as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import gensim.downloader as gensimapi\n",
    "from collections import Counter, defaultdict\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Cross Lingual Aspect Based Sentiment Analysis- Part 1.ipynb'\r\n",
      "'Cross Lingual Aspect Based Sentiment Analysis- Part 2.ipynb'\r\n",
      " Keras.ipynb\r\n",
      "'Multilingual  semeval  dataset creation.ipynb'\r\n",
      " results\r\n",
      "'Tatoeba Similarity Search.ipynb'\r\n",
      " utils\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_folder = \"../../../corpus/new/preprocess_from_isuru/\"\n",
    "# lankadeepa_data_path = data_folder + 'lankadeepa_tagged_comments.csv'\n",
    "# gossip_lanka_data_path = data_folder + 'gossip_lanka_tagged_comments.csv'\n",
    "\n",
    "# lankadeepa_data = pd.read_csv(lankadeepa_data_path)[:9059]\n",
    "# gossipLanka_data = pd.read_csv(gossip_lanka_data_path)\n",
    "# gossipLanka_data = gossipLanka_data.drop(columns=['Unnamed: 3'])\n",
    "\n",
    "# sinhala_data = pd.concat([lankadeepa_data,gossipLanka_data], ignore_index=True)\n",
    "# sinhala_data.to_csv(\"../data/sinhala_comments.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_aspects_ungrp = pd.read_csv('../data/Train_english_restaurants_ungrp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_aspects_ungrp = pd.read_csv('../data/Train_english_restaurants_ungrp.csv')\n",
    "val_aspects_ungrp2 = pd.read_csv('../data/Valid_english_restaurants_ungrp.csv')\n",
    "du_aspects_ungrp2  = pd.read_csv('../data/Dutch_restaurants_ungrp.csv')\n",
    "sp_aspects_ungrp2  = pd.read_csv('../data/Spanish_english_restaurants_ungrp.csv')\n",
    "\n",
    "sinhala_comments  = pd.read_csv('../data/sinhala_comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "sinhala_comments = sinhala_comments.loc[sinhala_comments['label'].isin([2, 3, 4])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2 7665]\n",
      " [   3 2403]\n",
      " [   4 3080]]\n"
     ]
    }
   ],
   "source": [
    "(unique, counts) = np.unique(sinhala_comments[\"label\"].values, return_counts = True)\n",
    "frequencies = np.asarray((unique, counts)).T\n",
    "print(frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_aspects_ungrp[['text']].to_csv('../data/processed/en_resturant.csv' , header = None , index = None , mode = 'w')\n",
    "val_aspects_ungrp2[['text']].to_csv('../data/processed/en_val.csv' , header = None , index = None , mode = 'w')\n",
    "du_aspects_ungrp2[['text']].to_csv('../data/processed/nl_resturant.csv' , header = None , index = None , mode = 'w')\n",
    "sp_aspects_ungrp2[['text']].to_csv('../data/processed/es_resturant.csv' , header = None , index = None , mode = 'w')\n",
    "\n",
    "sinhala_comments[['comment']].to_csv('../data/processed/sinhala_comments.csv' , header = None , index = None , mode = 'w')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "LASER_PATH = \"/home/lahiru/Projects/FYP/LASER/LASER\"\n",
    "sys.path.insert(0, LASER_PATH + '/source/lib')\n",
    "sys.path.insert(1, LASER_PATH + '/source')\n",
    "\n",
    "DATA_PATH = Path(\"../data/processed/\")\n",
    "CACHE_PATH = Path(\"../cache2/\")\n",
    "CACHE_PATH.mkdir(exist_ok=True)\n",
    "MODEL_PATH = Path(LASER_PATH + \"/models\")\n",
    "\n",
    "os.environ[\"LASER\"] = LASER_PATH \n",
    "SPACE_NORMALIZER = re.compile(\"\\s+\")\n",
    "Batch = namedtuple('Batch', 'srcs tokens lengths')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from indexing import IndexLoad, IndexTextOpen, IndexTextQuery, IndexSearchKNN, IndexCreate, IndexSearchMultiple\n",
    "from embed import SentenceEncoder, EncodeLoad, EncodeFile\n",
    "from text_processing import Token, BPEfastApply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Tokenizer: en_resturant.csv exists already\n",
      " - Tokenizer: nl_resturant.csv exists already\n",
      " - Tokenizer: es_resturant.csv exists already\n",
      " - Tokenizer: en_val.csv exists already\n"
     ]
    }
   ],
   "source": [
    "encoder = SentenceEncoder(\n",
    "    str(MODEL_PATH / \"bilstm.93langs.2018-12-26.pt\"),\n",
    "    max_sentences=None,\n",
    "    max_tokens=10000,\n",
    "    cpu=False)\n",
    "#original : cpu = False\n",
    "\n",
    "bpe_codes = str(MODEL_PATH / \"93langs.fcodes\")\n",
    "\n",
    "for lang in (\"en\" ,\"nl\", 'es'): \n",
    "    Token(\n",
    "        str(DATA_PATH / f\"{lang}_resturant.csv\"), ##english_resturant.txt\n",
    "        str(CACHE_PATH / f\"{lang}_resturant.csv\"),\n",
    "        lang=lang,\n",
    "        romanize=False,\n",
    "        lower_case=True, gzip=False,\n",
    "        verbose=True)\n",
    "    BPEfastApply(\n",
    "        str(CACHE_PATH / f\"{lang}_resturant.csv\"),\n",
    "        str(CACHE_PATH / f\"{lang}_resturant.bpe\"),\n",
    "        bpe_codes,\n",
    "        verbose=True, over_write=True)\n",
    "    EncodeFile(\n",
    "        encoder,\n",
    "        str(CACHE_PATH / f\"{lang}_resturant.bpe\"),\n",
    "        str(CACHE_PATH / f\"{lang}_resturant.enc\"),\n",
    "        verbose=True, over_write=True)    \n",
    "    \n",
    "    \n",
    "Token(\n",
    "    str(DATA_PATH / f\"en_val.csv\"), ##english_resturant.txt\n",
    "    str(CACHE_PATH / f\"en_val.csv\"),\n",
    "    lang=lang,\n",
    "    romanize=False,\n",
    "    lower_case=True, gzip=False,\n",
    "    verbose=True)\n",
    "BPEfastApply(\n",
    "    str(CACHE_PATH / f\"en_val.csv\"),\n",
    "    str(CACHE_PATH / f\"en_val.bpe\"),\n",
    "    bpe_codes,\n",
    "    verbose=True, over_write=True)\n",
    "EncodeFile(\n",
    "    encoder,\n",
    "    str(CACHE_PATH / f\"en_val.bpe\"),\n",
    "    str(CACHE_PATH / f\"en_val.enc\"),\n",
    "    verbose=True, over_write=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Tokenizer: sinhala_comments.csv in language sin  \n",
      " - fast BPE: processing sinhala_comments.csv\n",
      " - Encoder: sinhala_comments.bpe to sinhala_comments.enc\n",
      " - Encoder: 13148 sentences in 370s\n"
     ]
    }
   ],
   "source": [
    "Token(\n",
    "    str(DATA_PATH / f\"sinhala_comments.csv\"), ##english_resturant.txt\n",
    "    str(CACHE_PATH / f\"sinhala_comments.csv\"),\n",
    "    lang=\"sin\",\n",
    "    romanize=False,\n",
    "    lower_case=True, gzip=False,\n",
    "    verbose=True)\n",
    "BPEfastApply(\n",
    "    str(CACHE_PATH / f\"sinhala_comments.csv\"),\n",
    "    str(CACHE_PATH / f\"sinhala_comments.bpe\"),\n",
    "    bpe_codes,\n",
    "    verbose=True, over_write=True)\n",
    "EncodeFile(\n",
    "    encoder,\n",
    "    str(CACHE_PATH / f\"sinhala_comments.bpe\"),\n",
    "    str(CACHE_PATH / f\"sinhala_comments.enc\"),\n",
    "    verbose=True, over_write=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "LASER_PATH = \"/home/lahiru/Projects/FYP/LASER/LASER\"\n",
    "sys.path.insert(0, LASER_PATH + '/source/lib')\n",
    "sys.path.insert(1, LASER_PATH + '/source')\n",
    "\n",
    "DATA_PATH = Path(\"../data/processed/\")\n",
    "CACHE_PATH = Path(\"../cache2/\")\n",
    "CACHE_PATH.mkdir(exist_ok=True)\n",
    "MODEL_PATH = Path(LASER_PATH + \"/models\")\n",
    "\n",
    "os.environ[\"LASER\"] = LASER_PATH \n",
    "SPACE_NORMALIZER = re.compile(\"\\s+\")\n",
    "Batch = namedtuple('Batch', 'srcs tokens lengths')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - embedding: ../cache2/en_resturant.enc 1656 examples of dim 1024\n",
      " - creating FAISS index\n",
      " - embedding: ../cache2/en_val.enc 283 examples of dim 1024\n",
      " - creating FAISS index\n",
      " - embedding: ../cache2/nl_resturant.enc 960 examples of dim 1024\n",
      " - creating FAISS index\n",
      " - embedding: ../cache2/es_resturant.enc 1416 examples of dim 1024\n",
      " - creating FAISS index\n",
      " - embedding: ../cache2/sinhala_comments.enc 13148 examples of dim 1024\n",
      " - creating FAISS index\n"
     ]
    }
   ],
   "source": [
    "train_en, index_tr_en = IndexCreate(\n",
    "    str(CACHE_PATH / \"en_resturant.enc\"), 'FlatL2', verbose=True, save_index=False)\n",
    "val_en, index_val_en = IndexCreate(\n",
    "    str(CACHE_PATH / \"en_val.enc\"), 'FlatL2', verbose=True, save_index=False)\n",
    "\n",
    "data_du, index_du = IndexCreate(\n",
    "    str(CACHE_PATH / \"nl_resturant.enc\"), 'FlatL2', verbose=True, save_index=False)\n",
    "data_spanish, index_spanish = IndexCreate(\n",
    "    str(CACHE_PATH / \"es_resturant.enc\"), 'FlatL2', verbose=True, save_index=False)\n",
    "\n",
    "data_sinhala, index_sinhala = IndexCreate(\n",
    "    str(CACHE_PATH / \"sinhala_comments.enc\"), 'FlatL2', verbose=True, save_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_target(x):\n",
    "    if x=='positive':\n",
    "        return 2\n",
    "    elif x =='negative':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0 \n",
    "    \n",
    "def change_target_sinala(x):\n",
    "    if x== 4:\n",
    "        return 2\n",
    "    elif x == 2:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0 \n",
    "\n",
    "train_aspects_ungrp['polarities'] = train_aspects_ungrp['polarities'].apply(lambda x: change_target(x))\n",
    "val_aspects_ungrp2['polarities'] = val_aspects_ungrp2['polarities'].apply(lambda x: change_target(x))\n",
    "sp_aspects_ungrp2['polarities'] = sp_aspects_ungrp2['polarities'].apply(lambda x: change_target(x))\n",
    "du_aspects_ungrp2['polarities'] = du_aspects_ungrp2['polarities'].apply(lambda x: change_target(x))\n",
    "\n",
    "sinhala_comments['label'] = sinhala_comments['label'].apply(lambda x: change_target_sinala(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1656,), (283,), (960,), (1416,), (13148,))"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "tr_eng =  train_aspects_ungrp['polarities'].values\n",
    "val_eng =val_aspects_ungrp2['polarities'].values  \n",
    "y_du  = du_aspects_ungrp2['polarities'].values\n",
    "y_spainish  = sp_aspects_ungrp2['polarities'].values\n",
    "\n",
    "y_sinhala  = sinhala_comments['label'].values\n",
    "tr_eng.shape , val_eng.shape , y_du.shape , y_spainish.shape, y_sinhala.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0 2403]\n",
      " [   1 7665]\n",
      " [   2 3080]]\n"
     ]
    }
   ],
   "source": [
    "(unique, counts) = np.unique(y_sinhala, return_counts = True)\n",
    "frequencies = np.asarray((unique, counts)).T\n",
    "print(frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_eng = pd.get_dummies(tr_eng).values\n",
    "val_eng = pd.get_dummies(val_eng).values\n",
    "y_du = pd.get_dummies(y_du).values\n",
    "y_spainish = pd.get_dummies(y_spainish).values\n",
    "y_sinhala = pd.get_dummies(y_sinhala).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1], dtype=uint8)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_eng[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "std_scale = StandardScaler().fit(train_en)\n",
    "train_std = std_scale.transform(train_en) \n",
    "val_std = std_scale.transform(val_en)\n",
    "dutch_std = std_scale.transform(data_du)\n",
    "spanish_std = std_scale.transform(data_spanish)\n",
    "sinhala_std = std_scale.transform(data_sinhala)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(sinhala_std, y_sinhala, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN_model():\n",
    "    main_input = Input(shape=(1,1024 ), dtype='float', name='main_input')\n",
    "#     embedding  = Embedding(MAX_FEATURES, EMBEDDING_SIZE\n",
    "#               , input_length=MAX_LEN,\n",
    "#               name='embedding' ,trainable=False)(main_input)\n",
    "\n",
    "    embedding = Dropout(DROPOUT_VALUE_1)(main_input)\n",
    "\n",
    "    x = RNN(HIDDEN_DIMS)(embedding)\n",
    "\n",
    "    x = Dense(HIDDEN_DIMS, activation='relu', init='he_normal', \n",
    "              W_constraint = maxnorm(3), b_constraint=maxnorm(3),\n",
    "              name='mlp')(x)\n",
    "\n",
    "    x = Dropout(DROPOUT_VALUE_2, name='drop')(x)\n",
    "\n",
    "    output = Dense(3, init='he_normal',\n",
    "                   activation='softmax', name='output')(x)\n",
    "\n",
    "    model = Model(input=main_input, output=output ,name=\"RNN_model\")\n",
    "\n",
    "    model.compile(loss={'output':'categorical_crossentropy'},\n",
    "              optimizer=Adadelta(lr=0.95, epsilon=1e-06),\n",
    "              metrics=[\"accuracy\",\n",
    "                       tf.keras.metrics.Precision(),\n",
    "                        tf.keras.metrics.Recall(),\n",
    "                       f1])\n",
    "    \n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "def simple_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Dense(512))\n",
    "    model.add(tf.keras.layers.Dense(256))\n",
    "    model.add(tf.keras.layers.Dense(3))\n",
    "    model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "    # This builds the model for the first time:\n",
    "    \n",
    "    return model\n",
    "\n",
    "def simple_model2():\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(32, input_shape=(1024, 1024)))\n",
    "    model.add(Dense(3))\n",
    "    print(model.summary())\n",
    "    model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "    return model\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "('Keyword argument not understood:', 'init')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-147-1e5776180f84>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRNN_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_std\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_eng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-146-ae585b7faef4>\u001b[0m in \u001b[0;36mRNN_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     x = Dense(HIDDEN_DIMS, activation='relu', init='he_normal', \n\u001b[1;32m     12\u001b[0m               \u001b[0mW_constraint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaxnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_constraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaxnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m               name='mlp')(x)\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDROPOUT_VALUE_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'drop'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lahiru/anaconda3/envs/LASER/lib/python3.6/site-packages/tensorflow/python/keras/layers/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, units, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, **kwargs)\u001b[0m\n\u001b[1;32m   1142\u001b[0m                **kwargs):\n\u001b[1;32m   1143\u001b[0m     super(Dense, self).__init__(\n\u001b[0;32m-> 1144\u001b[0;31m         activity_regularizer=activity_regularizer, **kwargs)\n\u001b[0m\u001b[1;32m   1145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0munits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lahiru/anaconda3/envs/LASER/lib/python3.6/site-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lahiru/anaconda3/envs/LASER/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, trainable, name, dtype, dynamic, **kwargs)\u001b[0m\n\u001b[1;32m    316\u001b[0m     }\n\u001b[1;32m    317\u001b[0m     \u001b[0;31m# Validate optional keyword arguments.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m     \u001b[0mgeneric_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallowed_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;31m# Mutable properties\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lahiru/anaconda3/envs/LASER/lib/python3.6/site-packages/tensorflow/python/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mvalidate_kwargs\u001b[0;34m(kwargs, allowed_kwargs, error_message)\u001b[0m\n\u001b[1;32m    776\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mkwarg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkwarg\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mallowed_kwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 778\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_message\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwarg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    779\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: ('Keyword argument not understood:', 'init')"
     ]
    }
   ],
   "source": [
    "model = RNN_model()\n",
    "model.fit(train_std, tr_eng, batch_size=64, epochs=10, validation_data = (X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_Model(model,X_train, y_train, cross_validation = False):\n",
    "\n",
    "  print('Training and Testing...')\n",
    "  \n",
    "  es = EarlyStopping(monitor='val_f1', mode='max', verbose=1, patience=5)\n",
    "  checkpoint = ModelCheckpoint(model_save_path, monitor='val_f1', verbose=1, save_best_only=True, mode='max')\n",
    "  callbacks_list = [checkpoint,es]\n",
    "\n",
    "  if (cross_validation):\n",
    "    callbacks_list = [es]\n",
    "  \n",
    "  his = model.fit(X_train, y_train, validation_split=VALIDATION_SPLIT, epochs=NB_EPOCHS, batch_size=BATCH_SIZE, callbacks=callbacks_list, verbose=1)\n",
    "  return model,his"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_FEATURES = 300\n",
    "VERBOSITY = 1\n",
    "VALIDATION_SPLIT = 0.1\n",
    "NB_EPOCHS = 20\n",
    "FOLDS = 10\n",
    "EMBEDDING_SIZE = 300\n",
    "\n",
    "BATCH_SIZE = 32 # 64, 128\n",
    "NB_FILTERS = 200 #200\n",
    "FILTER_LENGTH = 4 # test with 2,3,4,5\n",
    "# HIDDEN_DIMS = NB_FILTERS * 2\n",
    "HIDDEN_DIMS = 32\n",
    "MAX_LEN = 210 #test with other values(only this value work for now)\n",
    "DROPOUT_VALUE_1 = 0.5 #0.8 #0.3\n",
    "DROPOUT_VALUE_2 = 0.5\n",
    "L2_REG= 0.01\n",
    "\n",
    "RNN = LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
